[
["index.html", "Data exploration and statistics with R and the tidyverse Chapter 1 Foreword", " Data exploration and statistics with R and the tidyverse Vivien Roussez (A1 Telekom Austria Group) Fall 2020 Chapter 1 Foreword Welcome to this learning week in Asigmo’s program ! This week will be dedicated to data exploration, manipulation and visualization using R and the tidyverse. To that end, we will also cover the basics of statistics, that are essential for your understanding of modeling and later on machine learning ! We will deal with complex and pretty dirty data, but this is real-world data, and you will also see that, more than artificial intelligence, you need common sense to deal with data ! This material was powered by bookdown To contact me : vivien.roussez@gmail.com "],
["intro.html", "Chapter 2 Introduction 2.1 Topics of the week 2.2 The data science workflow 2.3 Resources 2.4 Data of the week", " Chapter 2 Introduction 2.1 Topics of the week During this week, we will cover a lot of basic and nonetheless indispensable tools for a data scientist. We will cover mainly the “boring side” of data science (aka data analysis :P ). As a matter of fact, if machine learning is more and more automatized, everything that is related to data exploration, wrangling, cleaning, understanding and analysis is hardly doable by “AI”. What’s on our agenda : Introduction to R Data manipulation Introduction to statistics Data exploration Explainable machine learning (eg regression) Data visualization Data cleaning Dimension reduction Important notes : of course, it’s not a linear path and the data science workflow is not a simple execution of those steps in a pre-determined order. It’s all connected and you’ll have to move back and forth between all of them to achieve your goal. And this goal, what is it already ? in data science is the word “science”. And what is science ? Huge topic… But a few keywords that you should always have in mind when starting a project reproducibility hypothesis testing incremental iterative monitored 2.2 The data science workflow A popular schematic overview of a the workflow 2.3 Resources You will find a lot of resources online. Here is a selection, mostly related to R. However, our goal for this session is to have you understand how all the methods we will cover are related to each other and when you should consider using them R for data science Statistics and econometrics Statistics and exploration with R From data to viz (featuring python :D ) Data visualization More resource online books using R (text mining, machine learning…) 2.4 Data of the week In this course, we will handle with my own personal data (I give my consent ! ;) ) Those are my sports activity data, which I got from garmin (thanks to the GDPR). If you want to get your own data from this page. Being a triathlete, there are several sports involved and a lot of activities. The goal will be to import, clean, analyse the data with statistics and eventually build some first models (explainable AI). The export results in a lot of JSON files and we will focus on the summary data. It’s real worl data and you’ll see it’s complicated, dirty and requires a lot of preparation/manipulation ! require(tidyverse) dat &lt;- read.csv(&quot;Data/Sports/Activities.csv&quot;,header = T) head(dat) ## activityId uuidMsb uuidLsb name activityType ## 1 5570974040 3.695224e+18 -7.098507e+18 En piscine lap_swimming ## 2 5566524321 -5.212790e+18 -8.140623e+18 Vienna Cyclisme cycling ## 3 5561266034 1.271356e+18 -5.522844e+18 Korneuburg Cyclisme cycling ## 4 5555881653 -2.923938e+18 -4.676832e+18 Vienna Course running ## 5 5551811953 7.859043e+18 -7.909532e+18 Zwift - London virtual_ride ## 6 5551052200 -9.554086e+17 -6.866435e+18 En piscine lap_swimming ## userProfileId timeZoneId beginTimestamp eventTypeId rule sportType ## 1 1141258 124 1.600700e+12 9 public GENERIC ## 2 1141258 124 1.600602e+12 9 public CYCLING ## 3 1141258 124 1.600523e+12 9 public CYCLING ## 4 1141258 124 1.600439e+12 9 public RUNNING ## 5 1141258 124 1.600362e+12 9 public GENERIC ## 6 1141258 124 1.600353e+12 9 public GENERIC ## startTimeGmt startTimeLocal duration distance elevationGain elevationLoss ## 1 1.600700e+12 1.600708e+12 3640221 300000 NA NA ## 2 1.600602e+12 1.600609e+12 15064570 12293220 200100 196800 ## 3 1.600523e+12 1.600530e+12 10382433 8360927 74500 74200 ## 4 1.600439e+12 1.600447e+12 5393830 1792890 42600 42500 ## 5 1.600362e+12 1.600369e+12 3626825 3492347 16000 0 ## 6 1.600353e+12 1.600360e+12 3359145 285000 NA NA ## avgSpeed maxSpeed avgHr maxHr calories startLongitude startLatitude ## 1 0.0986 0.1122 NA NA 2547.532 NA NA ## 2 0.8160 1.9315 144 176 16345.268 16.31524 48.20915 ## 3 0.8053 1.9557 117 161 8744.572 16.33986 48.34994 ## 4 0.3324 1.2475 155 176 4923.274 16.31587 48.21432 ## 5 0.9629 1.6364 138 163 3083.855 0.00000 0.00000 ## 6 0.1011 0.3524 NA NA 2505.632 NA NA ## aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 1 NA 0.0000 0 ## 2 3.5 0.0000 0 ## 3 2.4 0.0000 0 ## 4 3.0 0.1875 0 ## 5 0.0 0.0000 0 ## 6 NA 0.0000 0 ## elapsedDuration movingDuration anaerobicTrainingEffect deviceId ## 1 3955901 3099653 NA 3968818126 ## 2 16176575 15000179 0.0 3968818126 ## 3 11319866 10356078 0.0 3968818126 ## 4 5538505 5389603 0.2 3968818126 ## 5 3629000 3611000 NA 3825981698 ## 6 3585826 2858951 NA 3968818126 ## minTemperature maxTemperature minElevation maxElevation locationName ## 1 25 26 NA NA &lt;NA&gt; ## 2 19 29 18480 95480 Vienna ## 3 18 29 13980 32200 Korneuburg ## 4 19 27 24920 54400 Vienna ## 5 NA NA 300 3420 City of Westminster ## 6 26 27 NA NA &lt;NA&gt; ## maxVerticalSpeed lapCount endLongitude endLatitude activeSets totalSets ## 1 NA 34 NA NA NA NA ## 2 0.43999939 25 16.25314 48.20399 NA NA ## 3 0.34000092 17 16.38793 48.38009 NA NA ## 4 0.08000183 18 16.31980 48.22264 NA NA ## 5 0.16000004 1 NA NA NA NA ## 6 NA 32 NA NA NA NA ## totalReps purposeful autoCalcCalories favorite pr elevationCorrected ## 1 NA FALSE FALSE FALSE FALSE FALSE ## 2 NA FALSE FALSE FALSE FALSE FALSE ## 3 NA FALSE FALSE FALSE FALSE FALSE ## 4 NA FALSE FALSE FALSE FALSE FALSE ## 5 NA FALSE FALSE FALSE FALSE FALSE ## 6 NA FALSE FALSE FALSE FALSE FALSE ## atpActivity parent maxRunCadence steps avgVerticalOscillation ## 1 FALSE FALSE NA NA NA ## 2 FALSE FALSE NA NA NA ## 3 FALSE FALSE NA NA NA ## 4 FALSE FALSE 104 15620 NA ## 5 FALSE FALSE NA NA NA ## 6 FALSE FALSE NA NA NA ## avgGroundContactTime avgStrideLength vO2MaxValue avgVerticalRatio ## 1 NA NA NA NA ## 2 NA NA 72 NA ## 3 NA NA 71 NA ## 4 NA 115.6389 58 NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence avgPower ## 1 NA NA NA NA ## 2 NA NA NA 260 ## 3 NA NA NA 202 ## 4 NA 172.375 208 NA ## 5 NA NA NA 212 ## 6 NA NA NA NA ## avgBikeCadence maxBikeCadence strokes normPower avgLeftBalance ## 1 NA NA 1198 NA NA ## 2 83 114 17968 296.0000 49.92 ## 3 82 107 12571 231.0000 49.90 ## 4 NA NA NA NA NA ## 5 91 114 0 218.3049 NA ## 6 NA NA 1152 NA NA ## avgRightBalance max20MinPower trainingStressScore intensityFactor ## 1 NA NA NA NA ## 2 50.08 375.1583 291.4 0.835 ## 3 50.10 242.7692 122.8 0.653 ## 4 NA NA NA NA ## 5 NA 221.0525 NA NA ## 6 NA NA NA NA ## lactateThresholdBpm lactateThresholdSpeed avgStrokes activeLengths avgSwolf ## 1 NA NA 23.0 60 74 ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA 22.6 57 72 ## poolLength avgStrokeDistance avgSwimCadence maxSwimCadence maxFtp workoutId ## 1 5000 217 27 29 NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 5000 221 27 30 NA NA ## decoDive parentId avgVerticalSpeed maxDepth avgDepth surfaceInterval ## 1 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA ## floorsDescended bottomTime ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA "],
["intro-r.html", "Chapter 3 Introduction to R 3.1 What is R 3.2 Basic commands to know 3.3 Data structures in R", " Chapter 3 Introduction to R R is one of the most popular data science language along with Python and Julia 3.1 What is R 3.1.1 Description R is an open source programming language initially dedicated to statistics and data analysis. It is the open-source version of the original S/S-plus language, developed by Bell labs a looong time ago. It was developed in the late 90’s. Being open-source, the number of packages available is considerable, generating both completeness and confusion. R is a functional programming language, meaning that functions are at the very core of its usage. It is not a object-oriented language although there are classes, but which are mainly hidden from the end-user. THe basic R is a command line interface, pretty similar to the bash It is an interactive language, meaning you can execute commands one after the other, no compilation is needed to execute a sequence of commands and you can try / adjust yourt code on the fly \\(\\rightarrow\\) very flexible (like IPython) Another very interesting characteristic of the language is that it is by design vectorized, meaning operations are executed at once on vectors, without explicit loops, which makes it very effective (as long as you don’t loop…) 3.1.2 (Objective) comparison with Pyhton What they have in common : Both language are open source and come with a wide set of capabilities and a community Interactivity Data science development environment (Jupyter) Several ways to achieve the same task What differs : R is dedicated to data / python is a generic programming language Functional vs object oriented Analysis (R) vs final product (Py) orientation 3.1.3 What can I do with R R’s core relies in data manipulation and statistical analysis. But the community made it grow in many directions Read data from multiple sources (excel, text files, databases, big data infrastructures…) Machine learning and deep learning Data visualization Communication Publications … Usages I probably have no idea about !! 3.1.4 Quick presentation of the ecosystem The core functionalities are available with base R on CRAN. On top of that, you can install several IDEs, the most popular ones being Rstudio, jupyter or VSCode For this training, we will use the R kernel of google collabs, but for many purposes, you’ll have to use another IDE (shiny, markdown…). This kernel comes with basics packages AND the tidyverse To add features to R, you’ll have then to install packages. Generally, when facing a problem (eg : I have to implement a naive Bayes estimator), you google it adding r at the beginning of the query and you’ll get the name of the packages that allow you to do that. Then you can install and activate it. # install.packages(&quot;e1071&quot;) library(e1071) Note : You can also call functions from an installed package without loading the whole package with ::. You might prefer this solution in several cases : You use only one function from the package only once \\(\\rightarrow\\) maybe not necessary to load everything Function names can be common across packages (eg: intersect, summarise…) using :: ensures you are using the function from the package you meant. Drawback : when not appearing at the beginning of the script, it can be unseen (for a new user) that the script requires such package to be installed dplyr::summarise(iris,n_species=dplyr::n_distinct(Species)) ## n_species ## 1 3 To find more information about R and its functinalities / latest news : R bloggers tidyverse.org twitter : #rstat Rstudio website Now you can use all functions of this packages ! 3.2 Basic commands to know Where to find help : Search engine to know how to do something help(lm) or ?lm to get help about a specific function (its inputs and output) stackoverflow to debug List the objects in memory ls() What is the current directory getwd() ; change it setwd() Browse folders and files dir() Session information (loaded packages and so on) sessionInfo() Install and load packages : see above View the source code of a function : lm Create a new object and assign a value to it &lt;-. display in the console by typing its name Commented lines, like in Python, start with a # ?lm ls() ## [1] &quot;dat&quot; sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] e1071_1.7-3 forcats_0.5.0 stringr_1.4.0 dplyr_1.0.2 ## [5] purrr_0.3.4 readr_1.3.1 tidyr_1.1.2 tibble_3.0.3 ## [9] ggplot2_3.3.2 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_1.1.0 xfun_0.17 haven_2.3.1 colorspace_1.4-1 ## [5] vctrs_0.3.4 generics_0.0.2 htmltools_0.5.0 yaml_2.2.1 ## [9] blob_1.2.1 rlang_0.4.7 pillar_1.4.6 glue_1.4.2 ## [13] withr_2.2.0 DBI_1.1.0 dbplyr_1.4.4 modelr_0.1.8 ## [17] readxl_1.3.1 lifecycle_0.2.0 munsell_0.5.0 gtable_0.3.0 ## [21] cellranger_1.1.0 rvest_0.3.6 evaluate_0.14 knitr_1.29 ## [25] class_7.3-17 fansi_0.4.1 broom_0.7.0 Rcpp_1.0.5 ## [29] scales_1.1.1 backports_1.1.10 jsonlite_1.7.1 fs_1.5.0 ## [33] hms_0.5.3 digest_0.6.25 stringi_1.5.3 bookdown_0.20 ## [37] grid_4.0.2 cli_2.0.2 tools_4.0.2 magrittr_1.5 ## [41] crayon_1.3.4 pkgconfig_2.0.3 ellipsis_0.3.1 xml2_1.3.2 ## [45] reprex_0.3.0 lubridate_1.7.9 assertthat_0.2.1 rmarkdown_2.3 ## [49] httr_1.4.2 rstudioapi_0.11 R6_2.4.1 compiler_4.0.2 getwd() ## [1] &quot;/Users/vivienroussez/Documents/Datascience/Asigmo/DataExploration&quot; dir(&quot;/&quot;) ## [1] &quot;Applications&quot; &quot;bin&quot; &quot;cores&quot; &quot;dev&quot; &quot;etc&quot; ## [6] &quot;home&quot; &quot;Library&quot; &quot;opt&quot; &quot;private&quot; &quot;sbin&quot; ## [11] &quot;System&quot; &quot;tmp&quot; &quot;Users&quot; &quot;usr&quot; &quot;var&quot; ## [16] &quot;Volumes&quot; lm ## function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, ## model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, ## contrasts = NULL, offset, ...) ## { ## ret.x &lt;- x ## ret.y &lt;- y ## cl &lt;- match.call() ## mf &lt;- match.call(expand.dots = FALSE) ## m &lt;- match(c(&quot;formula&quot;, &quot;data&quot;, &quot;subset&quot;, &quot;weights&quot;, &quot;na.action&quot;, ## &quot;offset&quot;), names(mf), 0L) ## mf &lt;- mf[c(1L, m)] ## mf$drop.unused.levels &lt;- TRUE ## mf[[1L]] &lt;- quote(stats::model.frame) ## mf &lt;- eval(mf, parent.frame()) ## if (method == &quot;model.frame&quot;) ## return(mf) ## else if (method != &quot;qr&quot;) ## warning(gettextf(&quot;method = &#39;%s&#39; is not supported. Using &#39;qr&#39;&quot;, ## method), domain = NA) ## mt &lt;- attr(mf, &quot;terms&quot;) ## y &lt;- model.response(mf, &quot;numeric&quot;) ## w &lt;- as.vector(model.weights(mf)) ## if (!is.null(w) &amp;&amp; !is.numeric(w)) ## stop(&quot;&#39;weights&#39; must be a numeric vector&quot;) ## offset &lt;- model.offset(mf) ## mlm &lt;- is.matrix(y) ## ny &lt;- if (mlm) ## nrow(y) ## else length(y) ## if (!is.null(offset)) { ## if (!mlm) ## offset &lt;- as.vector(offset) ## if (NROW(offset) != ny) ## stop(gettextf(&quot;number of offsets is %d, should equal %d (number of observations)&quot;, ## NROW(offset), ny), domain = NA) ## } ## if (is.empty.model(mt)) { ## x &lt;- NULL ## z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, ## ncol(y)) else numeric(), residuals = y, fitted.values = 0 * ## y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != ## 0) else ny) ## if (!is.null(offset)) { ## z$fitted.values &lt;- offset ## z$residuals &lt;- y - offset ## } ## } ## else { ## x &lt;- model.matrix(mt, mf, contrasts) ## z &lt;- if (is.null(w)) ## lm.fit(x, y, offset = offset, singular.ok = singular.ok, ## ...) ## else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, ## ...) ## } ## class(z) &lt;- c(if (mlm) &quot;mlm&quot;, &quot;lm&quot;) ## z$na.action &lt;- attr(mf, &quot;na.action&quot;) ## z$offset &lt;- offset ## z$contrasts &lt;- attr(x, &quot;contrasts&quot;) ## z$xlevels &lt;- .getXlevels(mt, mf) ## z$call &lt;- cl ## z$terms &lt;- mt ## if (model) ## z$model &lt;- mf ## if (ret.x) ## z$x &lt;- x ## if (ret.y) ## z$y &lt;- y ## if (!qr) ## z$qr &lt;- NULL ## z ## } ## &lt;bytecode: 0x7fc549162070&gt; ## &lt;environment: namespace:stats&gt; obj &lt;- 3 obj ## [1] 3 3.3 Data structures in R As mentioned, R is a functional programming language, which means that you will always call… functions. And functions are defined by parameters : the inputs you have to provide the function so that it can do what it’s meant for the result : the output you get. Stricly speacking, the result of a function is unique (as opposed to procedures). Of course, depending on the class of the result, it may of course be composite This chapter gives you some keys to understand and explore the results as they are provided by the functions. 3.3.1 Basic data structures Before introducing the data structure, a short precision about types. Values are stored in data structures which partially depend on their type : Logical (TRUE or FALSE) Numerical (integer, continuous or complex) Character (strings or categories) R recognizes the type of the value and modify it dynamically (no need to declare the type and it can e changed). To force R to coerce values to another type, you can use the functions as.numeric, as.character, as.logical. Important note : NA stands for not available and is common to all type when a value is missing. You can have other missing values though for numerical variables : Nan (not a mumber) eg 0/0 Inf (infinity) eg log(0) Attention : NULL applies to objects (eg a matrix or a list) and not to values themselves 3.3.1.1 Vectors Vectors are the basic data structure : it is a unidimensional collection of values having the same type. There are a lot of ways to generate vectors : my_vect &lt;- c(1,2,19,1) ; print(my_vect) ## [1] 1 2 19 1 my_vect &lt;- 1:10 ; print(my_vect) ## [1] 1 2 3 4 5 6 7 8 9 10 my_vect &lt;- seq(-15,100,.1) ; print(my_vect) ## [1] -15.0 -14.9 -14.8 -14.7 -14.6 -14.5 -14.4 -14.3 -14.2 -14.1 -14.0 -13.9 ## [13] -13.8 -13.7 -13.6 -13.5 -13.4 -13.3 -13.2 -13.1 -13.0 -12.9 -12.8 -12.7 ## [25] -12.6 -12.5 -12.4 -12.3 -12.2 -12.1 -12.0 -11.9 -11.8 -11.7 -11.6 -11.5 ## [37] -11.4 -11.3 -11.2 -11.1 -11.0 -10.9 -10.8 -10.7 -10.6 -10.5 -10.4 -10.3 ## [49] -10.2 -10.1 -10.0 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 ## [61] -9.0 -8.9 -8.8 -8.7 -8.6 -8.5 -8.4 -8.3 -8.2 -8.1 -8.0 -7.9 ## [73] -7.8 -7.7 -7.6 -7.5 -7.4 -7.3 -7.2 -7.1 -7.0 -6.9 -6.8 -6.7 ## [85] -6.6 -6.5 -6.4 -6.3 -6.2 -6.1 -6.0 -5.9 -5.8 -5.7 -5.6 -5.5 ## [97] -5.4 -5.3 -5.2 -5.1 -5.0 -4.9 -4.8 -4.7 -4.6 -4.5 -4.4 -4.3 ## [109] -4.2 -4.1 -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 ## [121] -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 ## [133] -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 ## [145] -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 ## [157] 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 ## [169] 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 ## [181] 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 ## [193] 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 ## [205] 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 ## [217] 6.6 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 ## [229] 7.8 7.9 8.0 8.1 8.2 8.3 8.4 8.5 8.6 8.7 8.8 8.9 ## [241] 9.0 9.1 9.2 9.3 9.4 9.5 9.6 9.7 9.8 9.9 10.0 10.1 ## [253] 10.2 10.3 10.4 10.5 10.6 10.7 10.8 10.9 11.0 11.1 11.2 11.3 ## [265] 11.4 11.5 11.6 11.7 11.8 11.9 12.0 12.1 12.2 12.3 12.4 12.5 ## [277] 12.6 12.7 12.8 12.9 13.0 13.1 13.2 13.3 13.4 13.5 13.6 13.7 ## [289] 13.8 13.9 14.0 14.1 14.2 14.3 14.4 14.5 14.6 14.7 14.8 14.9 ## [301] 15.0 15.1 15.2 15.3 15.4 15.5 15.6 15.7 15.8 15.9 16.0 16.1 ## [313] 16.2 16.3 16.4 16.5 16.6 16.7 16.8 16.9 17.0 17.1 17.2 17.3 ## [325] 17.4 17.5 17.6 17.7 17.8 17.9 18.0 18.1 18.2 18.3 18.4 18.5 ## [337] 18.6 18.7 18.8 18.9 19.0 19.1 19.2 19.3 19.4 19.5 19.6 19.7 ## [349] 19.8 19.9 20.0 20.1 20.2 20.3 20.4 20.5 20.6 20.7 20.8 20.9 ## [361] 21.0 21.1 21.2 21.3 21.4 21.5 21.6 21.7 21.8 21.9 22.0 22.1 ## [373] 22.2 22.3 22.4 22.5 22.6 22.7 22.8 22.9 23.0 23.1 23.2 23.3 ## [385] 23.4 23.5 23.6 23.7 23.8 23.9 24.0 24.1 24.2 24.3 24.4 24.5 ## [397] 24.6 24.7 24.8 24.9 25.0 25.1 25.2 25.3 25.4 25.5 25.6 25.7 ## [409] 25.8 25.9 26.0 26.1 26.2 26.3 26.4 26.5 26.6 26.7 26.8 26.9 ## [421] 27.0 27.1 27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 28.1 ## [433] 28.2 28.3 28.4 28.5 28.6 28.7 28.8 28.9 29.0 29.1 29.2 29.3 ## [445] 29.4 29.5 29.6 29.7 29.8 29.9 30.0 30.1 30.2 30.3 30.4 30.5 ## [457] 30.6 30.7 30.8 30.9 31.0 31.1 31.2 31.3 31.4 31.5 31.6 31.7 ## [469] 31.8 31.9 32.0 32.1 32.2 32.3 32.4 32.5 32.6 32.7 32.8 32.9 ## [481] 33.0 33.1 33.2 33.3 33.4 33.5 33.6 33.7 33.8 33.9 34.0 34.1 ## [493] 34.2 34.3 34.4 34.5 34.6 34.7 34.8 34.9 35.0 35.1 35.2 35.3 ## [505] 35.4 35.5 35.6 35.7 35.8 35.9 36.0 36.1 36.2 36.3 36.4 36.5 ## [517] 36.6 36.7 36.8 36.9 37.0 37.1 37.2 37.3 37.4 37.5 37.6 37.7 ## [529] 37.8 37.9 38.0 38.1 38.2 38.3 38.4 38.5 38.6 38.7 38.8 38.9 ## [541] 39.0 39.1 39.2 39.3 39.4 39.5 39.6 39.7 39.8 39.9 40.0 40.1 ## [553] 40.2 40.3 40.4 40.5 40.6 40.7 40.8 40.9 41.0 41.1 41.2 41.3 ## [565] 41.4 41.5 41.6 41.7 41.8 41.9 42.0 42.1 42.2 42.3 42.4 42.5 ## [577] 42.6 42.7 42.8 42.9 43.0 43.1 43.2 43.3 43.4 43.5 43.6 43.7 ## [589] 43.8 43.9 44.0 44.1 44.2 44.3 44.4 44.5 44.6 44.7 44.8 44.9 ## [601] 45.0 45.1 45.2 45.3 45.4 45.5 45.6 45.7 45.8 45.9 46.0 46.1 ## [613] 46.2 46.3 46.4 46.5 46.6 46.7 46.8 46.9 47.0 47.1 47.2 47.3 ## [625] 47.4 47.5 47.6 47.7 47.8 47.9 48.0 48.1 48.2 48.3 48.4 48.5 ## [637] 48.6 48.7 48.8 48.9 49.0 49.1 49.2 49.3 49.4 49.5 49.6 49.7 ## [649] 49.8 49.9 50.0 50.1 50.2 50.3 50.4 50.5 50.6 50.7 50.8 50.9 ## [661] 51.0 51.1 51.2 51.3 51.4 51.5 51.6 51.7 51.8 51.9 52.0 52.1 ## [673] 52.2 52.3 52.4 52.5 52.6 52.7 52.8 52.9 53.0 53.1 53.2 53.3 ## [685] 53.4 53.5 53.6 53.7 53.8 53.9 54.0 54.1 54.2 54.3 54.4 54.5 ## [697] 54.6 54.7 54.8 54.9 55.0 55.1 55.2 55.3 55.4 55.5 55.6 55.7 ## [709] 55.8 55.9 56.0 56.1 56.2 56.3 56.4 56.5 56.6 56.7 56.8 56.9 ## [721] 57.0 57.1 57.2 57.3 57.4 57.5 57.6 57.7 57.8 57.9 58.0 58.1 ## [733] 58.2 58.3 58.4 58.5 58.6 58.7 58.8 58.9 59.0 59.1 59.2 59.3 ## [745] 59.4 59.5 59.6 59.7 59.8 59.9 60.0 60.1 60.2 60.3 60.4 60.5 ## [757] 60.6 60.7 60.8 60.9 61.0 61.1 61.2 61.3 61.4 61.5 61.6 61.7 ## [769] 61.8 61.9 62.0 62.1 62.2 62.3 62.4 62.5 62.6 62.7 62.8 62.9 ## [781] 63.0 63.1 63.2 63.3 63.4 63.5 63.6 63.7 63.8 63.9 64.0 64.1 ## [793] 64.2 64.3 64.4 64.5 64.6 64.7 64.8 64.9 65.0 65.1 65.2 65.3 ## [805] 65.4 65.5 65.6 65.7 65.8 65.9 66.0 66.1 66.2 66.3 66.4 66.5 ## [817] 66.6 66.7 66.8 66.9 67.0 67.1 67.2 67.3 67.4 67.5 67.6 67.7 ## [829] 67.8 67.9 68.0 68.1 68.2 68.3 68.4 68.5 68.6 68.7 68.8 68.9 ## [841] 69.0 69.1 69.2 69.3 69.4 69.5 69.6 69.7 69.8 69.9 70.0 70.1 ## [853] 70.2 70.3 70.4 70.5 70.6 70.7 70.8 70.9 71.0 71.1 71.2 71.3 ## [865] 71.4 71.5 71.6 71.7 71.8 71.9 72.0 72.1 72.2 72.3 72.4 72.5 ## [877] 72.6 72.7 72.8 72.9 73.0 73.1 73.2 73.3 73.4 73.5 73.6 73.7 ## [889] 73.8 73.9 74.0 74.1 74.2 74.3 74.4 74.5 74.6 74.7 74.8 74.9 ## [901] 75.0 75.1 75.2 75.3 75.4 75.5 75.6 75.7 75.8 75.9 76.0 76.1 ## [913] 76.2 76.3 76.4 76.5 76.6 76.7 76.8 76.9 77.0 77.1 77.2 77.3 ## [925] 77.4 77.5 77.6 77.7 77.8 77.9 78.0 78.1 78.2 78.3 78.4 78.5 ## [937] 78.6 78.7 78.8 78.9 79.0 79.1 79.2 79.3 79.4 79.5 79.6 79.7 ## [949] 79.8 79.9 80.0 80.1 80.2 80.3 80.4 80.5 80.6 80.7 80.8 80.9 ## [961] 81.0 81.1 81.2 81.3 81.4 81.5 81.6 81.7 81.8 81.9 82.0 82.1 ## [973] 82.2 82.3 82.4 82.5 82.6 82.7 82.8 82.9 83.0 83.1 83.2 83.3 ## [985] 83.4 83.5 83.6 83.7 83.8 83.9 84.0 84.1 84.2 84.3 84.4 84.5 ## [997] 84.6 84.7 84.8 84.9 85.0 85.1 85.2 85.3 85.4 85.5 85.6 85.7 ## [1009] 85.8 85.9 86.0 86.1 86.2 86.3 86.4 86.5 86.6 86.7 86.8 86.9 ## [1021] 87.0 87.1 87.2 87.3 87.4 87.5 87.6 87.7 87.8 87.9 88.0 88.1 ## [1033] 88.2 88.3 88.4 88.5 88.6 88.7 88.8 88.9 89.0 89.1 89.2 89.3 ## [1045] 89.4 89.5 89.6 89.7 89.8 89.9 90.0 90.1 90.2 90.3 90.4 90.5 ## [1057] 90.6 90.7 90.8 90.9 91.0 91.1 91.2 91.3 91.4 91.5 91.6 91.7 ## [1069] 91.8 91.9 92.0 92.1 92.2 92.3 92.4 92.5 92.6 92.7 92.8 92.9 ## [1081] 93.0 93.1 93.2 93.3 93.4 93.5 93.6 93.7 93.8 93.9 94.0 94.1 ## [1093] 94.2 94.3 94.4 94.5 94.6 94.7 94.8 94.9 95.0 95.1 95.2 95.3 ## [1105] 95.4 95.5 95.6 95.7 95.8 95.9 96.0 96.1 96.2 96.3 96.4 96.5 ## [1117] 96.6 96.7 96.8 96.9 97.0 97.1 97.2 97.3 97.4 97.5 97.6 97.7 ## [1129] 97.8 97.9 98.0 98.1 98.2 98.3 98.4 98.5 98.6 98.7 98.8 98.9 ## [1141] 99.0 99.1 99.2 99.3 99.4 99.5 99.6 99.7 99.8 99.9 100.0 my_vect &lt;- rnorm(100) ; print(my_vect) ## [1] 0.20977202 -0.40251906 0.19893819 -0.98240865 0.51354646 -1.82066103 ## [7] 1.25538464 -0.61534876 1.40495322 0.79418739 1.23644012 0.21921511 ## [13] -0.52282504 0.73152135 1.45415201 0.44792143 1.82673615 -0.37020070 ## [19] -0.55167752 0.94071110 1.00347492 -0.01240129 -0.81418478 -0.70598956 ## [25] 0.47982500 -0.29386622 0.94046164 -0.11904119 -1.76899037 1.13784042 ## [31] -0.07373035 -2.37060735 0.55239127 0.32582900 -0.13877673 -1.82487961 ## [37] -0.15201004 0.97476588 -0.88605669 0.30305886 -1.61060006 0.30568922 ## [43] 0.45480051 0.91257691 -1.46884630 0.25477365 0.42575996 -0.20496548 ## [49] 1.61751677 -0.17248963 1.17248286 -0.20316499 -1.13168995 1.15828941 ## [55] 1.18041639 -0.05711416 1.45609315 0.68606994 0.87879916 0.09195986 ## [61] -0.05736617 1.20604697 0.71875381 -0.40129763 -1.20231216 0.12907999 ## [67] 0.35582049 0.70230373 1.28564011 2.03805958 -0.41737317 1.04965401 ## [73] 1.80268993 1.19613481 1.08939821 0.72406254 0.96290019 1.00366762 ## [79] 0.71115966 -0.57377788 -0.08860625 -0.96516037 -0.15340914 -1.46696551 ## [85] 1.24542934 -0.54709402 -0.24406533 0.69276531 -0.21350631 1.53053907 ## [91] -0.14174015 -1.58226613 -0.21378029 0.04143796 0.05300747 -0.18224290 ## [97] 2.48791882 1.08308891 0.69732969 -1.22408319 my_vect &lt;- sample(letters,100,replace = T) ; print(my_vect) ## [1] &quot;y&quot; &quot;s&quot; &quot;z&quot; &quot;e&quot; &quot;a&quot; &quot;v&quot; &quot;m&quot; &quot;d&quot; &quot;l&quot; &quot;p&quot; &quot;a&quot; &quot;m&quot; &quot;u&quot; &quot;r&quot; &quot;c&quot; &quot;h&quot; &quot;m&quot; &quot;s&quot; ## [19] &quot;w&quot; &quot;k&quot; &quot;y&quot; &quot;r&quot; &quot;t&quot; &quot;d&quot; &quot;j&quot; &quot;r&quot; &quot;f&quot; &quot;q&quot; &quot;k&quot; &quot;q&quot; &quot;u&quot; &quot;f&quot; &quot;d&quot; &quot;q&quot; &quot;j&quot; &quot;c&quot; ## [37] &quot;d&quot; &quot;n&quot; &quot;u&quot; &quot;c&quot; &quot;p&quot; &quot;m&quot; &quot;v&quot; &quot;q&quot; &quot;p&quot; &quot;z&quot; &quot;s&quot; &quot;r&quot; &quot;l&quot; &quot;l&quot; &quot;g&quot; &quot;n&quot; &quot;x&quot; &quot;z&quot; ## [55] &quot;w&quot; &quot;t&quot; &quot;c&quot; &quot;d&quot; &quot;d&quot; &quot;b&quot; &quot;r&quot; &quot;n&quot; &quot;x&quot; &quot;w&quot; &quot;q&quot; &quot;w&quot; &quot;y&quot; &quot;t&quot; &quot;r&quot; &quot;c&quot; &quot;t&quot; &quot;w&quot; ## [73] &quot;k&quot; &quot;k&quot; &quot;q&quot; &quot;x&quot; &quot;l&quot; &quot;g&quot; &quot;l&quot; &quot;o&quot; &quot;x&quot; &quot;o&quot; &quot;n&quot; &quot;x&quot; &quot;v&quot; &quot;j&quot; &quot;j&quot; &quot;u&quot; &quot;q&quot; &quot;k&quot; ## [91] &quot;s&quot; &quot;m&quot; &quot;d&quot; &quot;i&quot; &quot;y&quot; &quot;f&quot; &quot;i&quot; &quot;s&quot; &quot;c&quot; &quot;h&quot; You can access vector values with integer indexes (that are vector themselves). Note : unlike Python, the indexes start with the value 1, not 0 ! my_vect[4] ## [1] &quot;e&quot; my_vect[1:4] ## [1] &quot;y&quot; &quot;s&quot; &quot;z&quot; &quot;e&quot; A vector can be named meaning that each element has a name through which it can be accessed. my_vect &lt;- 1:10 names(my_vect) &lt;- letters[1:10] ; print(my_vect) ## a b c d e f g h i j ## 1 2 3 4 5 6 7 8 9 10 my_vect[&quot;b&quot;] ## b ## 2 Did you notice you can assign values to a vector’s attribute ? :D 3.3.1.2 Matrices and arrays Matrices are a 2-dimensional collection of values having the same type. An array is an extension of matrices for more than 2 dimensions. mat &lt;- matrix(1,ncol=10,nrow = 15) ; print(mat) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 1 1 1 1 1 1 1 1 1 ## [2,] 1 1 1 1 1 1 1 1 1 1 ## [3,] 1 1 1 1 1 1 1 1 1 1 ## [4,] 1 1 1 1 1 1 1 1 1 1 ## [5,] 1 1 1 1 1 1 1 1 1 1 ## [6,] 1 1 1 1 1 1 1 1 1 1 ## [7,] 1 1 1 1 1 1 1 1 1 1 ## [8,] 1 1 1 1 1 1 1 1 1 1 ## [9,] 1 1 1 1 1 1 1 1 1 1 ## [10,] 1 1 1 1 1 1 1 1 1 1 ## [11,] 1 1 1 1 1 1 1 1 1 1 ## [12,] 1 1 1 1 1 1 1 1 1 1 ## [13,] 1 1 1 1 1 1 1 1 1 1 ## [14,] 1 1 1 1 1 1 1 1 1 1 ## [15,] 1 1 1 1 1 1 1 1 1 1 mat &lt;- matrix(1:5,ncol=5,nrow=7) ; print(mat) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 2 4 ## [2,] 2 4 1 3 5 ## [3,] 3 5 2 4 1 ## [4,] 4 1 3 5 2 ## [5,] 5 2 4 1 3 ## [6,] 1 3 5 2 4 ## [7,] 2 4 1 3 5 arr &lt;- array(1:10,dim = c(10,2,3)) ; print(arr) ## , , 1 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 ## ## , , 2 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 ## ## , , 3 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 matrix(rnorm(9),3,3) m1 &lt;- matrix(1,2,3) m2 &lt;- matrix(1,3,2) m1*m2 3.3.1.3 Lists Lists are a very versatile and convenient class that allows you to store heterogeneous values and data structures my_list &lt;- list(&quot;A&quot;,1,LETTERS[1:10],matrix(1,3,3)) ; my_list ## [[1]] ## [1] &quot;A&quot; ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; ## ## [[4]] ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 Like with vectors, list elements can be accessed via their index or their name. If a list has been named, you have something very similar to python dictionaries. In case the list is named, you can also access its elements via the $ operator. names(my_list) &lt;- paste0(&quot;thing&quot;,1:length(my_list)) my_list[1] ## $thing1 ## [1] &quot;A&quot; my_list[&quot;thing1&quot;] ## $thing1 ## [1] &quot;A&quot; my_list$thing3 ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; # my_list[2]*10 3.3.1.4 Dataframes A Dataframe is the most common data representation (think of an excel spreadsheet): it is made out of columns and rows like a matrix, but the columns can have different types. In R, Dataframes are natives (no need to install another package). They are basically a list of vectors that have the same length. Let’s have a look at Fisher’s iris dataframe (included in base R for demonstration purposes) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa to explore the content of a dataframe, you can of course print it, but if you want amore detailed overview of it, you can use the str or the glimpse functions str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... glimpse(iris) ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, … In general, str (for structure) is a very powerful function to explore the content of a data structure (see next part). To explore it further, you can use the following functions names(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## plot(iris) # don&#39;t do that with too big data of course ! 3.3.1.5 Functions As mentioned before, R is a functional programming langueage and you can of course create your own functions (which can be afterwards integrated in a package). To cfeate a function, the syntax is such : square &lt;- function(xx=2) # 2 is the default value (not mandatory) { res &lt;- xx^2 return(res) } # Shorthand # square &lt;- function(xx) xx^2 # Use it square() ## [1] 4 square(5) ## [1] 25 3.3.1.6 Exercices Create a vector mixing both numbers and strings : what happens ? Create a vector containing the values “fellow 1” to fellow 15\". Hint : be lazy and use the paste() function Replace the value “fellow 5” with “best fellow” Create a matrix (3,3) of random numbers drawn from a gaussian distribution. Create two numerical matrices of size resp (2,3) and (3,2) filled with 1s and compute their product From the previous list, the second element is a number ; multiply this number by 10 accessing it via its index Create a new list containing the previous list and some other random elements 3.3.2 Explore a new data structure (or object) You will often face new data structures resulting from new functions, and they will be more complicated than the ones we’ve just covered. Let us take the example of the linear regression (which we will cover in section 7) library(ggplot2) ggplot(iris,aes(Petal.Length,Sepal.Length)) + geom_jitter() + geom_smooth(method=&quot;lm&quot;) + theme_minimal() Spoiler alert : the regression aims to find \\(\\alpha\\) and \\(\\beta\\) such that an explained variable \\(y\\) can be expressed as \\(y = \\alpha \\cdot x + \\beta\\) where \\(x\\) is an explanatory variable. In R, to find the values of \\(\\alpha\\) and \\(\\beta\\), you will use the lmfunction. So let’s fit this model and print the result reg &lt;- lm(Petal.Length ~ Sepal.Length,data=iris) reg ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## ## Coefficients: ## (Intercept) Sepal.Length ## -7.101 1.858 Ok, that’s really minimal information… Let’s try to dig into this reg object to find more. names(reg) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; str(reg) ## List of 12 ## $ coefficients : Named num [1:2] -7.1 1.86 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; ## $ residuals : Named num [1:150] -0.9766 -0.6049 -0.3332 0.0527 -0.7907 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:150] -46.026 18.785 -0.207 0.184 -0.679 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:150] 2.38 2 1.63 1.45 2.19 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.08 1.09 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 148 ## $ xlevels : Named list() ## $ call : language lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language Petal.Length ~ Sepal.Length ## .. ..- attr(*, &quot;variables&quot;)= language list(Petal.Length, Sepal.Length) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## .. .. .. ..$ : chr &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(Petal.Length, Sepal.Length) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## $ model :&#39;data.frame&#39;: 150 obs. of 2 variables: ## ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## ..$ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language Petal.Length ~ Sepal.Length ## .. .. ..- attr(*, &quot;variables&quot;)= language list(Petal.Length, Sepal.Length) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## .. .. .. .. ..$ : chr &quot;Sepal.Length&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;Sepal.Length&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(Petal.Length, Sepal.Length) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; That’s more interesting ! It seems that I can get more, including raw data, residuals, coefficients, degrees of freedom… And in general, you can apply standard functions on it as well summary(reg) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 plot(reg) Ok, that’s it, I have almost all that I wanted ! We’ll cover the rest at the end of the week ! 3.3.2.1 Exercise On the iris dataset, use the kmeans function to cluster the flowers with respect to Sepal.Length and Petal.Length and try to find your way in the resulting object. "],
["manip.html", "Chapter 4 Data manipulation 4.1 Import 4.2 The grammar of data manipulation 4.3 Let’s import and wrangle some data ! 4.4 The split-apply-combine operation 4.5 Tidy your data", " Chapter 4 Data manipulation In order to manipulate and wrangle data, there are (at least) 3 frameworks available : Base R (not covered) : similar to Pandas Tidyverse/dplyr : high level interface data.table : less friendly user interface but amazingly optimized We’ll cover the tidyverse approach as it provides a very nice and coherent framework for more than data manipulation. The “tidy” comes an original paper from Hadley Wickham which sets those common sense principles Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. You can check further details and visualization on the R for data science online book Those principles have been largely adopted by the community (powered by Rstudio) and created a full parallel dialect in R for almost all data science tasks : the tidyverse which offers a coherent set of features that are, in addition, often nicely optimized (written in C++) 4.1 Import 4.1.1 Text files You can either use the basic read.table and related functions (eg read.csv) which work pretty fine. If the file is large, you might consider tools from other package such as read_csv and others from the readr package (part of the tidyverse) dat &lt;- read.csv(&quot;Data/Sports/Activities.csv&quot;,header = T) head(dat) ## activityId uuidMsb uuidLsb name activityType ## 1 5570974040 3.695224e+18 -7.098507e+18 En piscine lap_swimming ## 2 5566524321 -5.212790e+18 -8.140623e+18 Vienna Cyclisme cycling ## 3 5561266034 1.271356e+18 -5.522844e+18 Korneuburg Cyclisme cycling ## 4 5555881653 -2.923938e+18 -4.676832e+18 Vienna Course running ## 5 5551811953 7.859043e+18 -7.909532e+18 Zwift - London virtual_ride ## 6 5551052200 -9.554086e+17 -6.866435e+18 En piscine lap_swimming ## userProfileId timeZoneId beginTimestamp eventTypeId rule sportType ## 1 1141258 124 1.600700e+12 9 public GENERIC ## 2 1141258 124 1.600602e+12 9 public CYCLING ## 3 1141258 124 1.600523e+12 9 public CYCLING ## 4 1141258 124 1.600439e+12 9 public RUNNING ## 5 1141258 124 1.600362e+12 9 public GENERIC ## 6 1141258 124 1.600353e+12 9 public GENERIC ## startTimeGmt startTimeLocal duration distance elevationGain elevationLoss ## 1 1.600700e+12 1.600708e+12 3640221 300000 NA NA ## 2 1.600602e+12 1.600609e+12 15064570 12293220 200100 196800 ## 3 1.600523e+12 1.600530e+12 10382433 8360927 74500 74200 ## 4 1.600439e+12 1.600447e+12 5393830 1792890 42600 42500 ## 5 1.600362e+12 1.600369e+12 3626825 3492347 16000 0 ## 6 1.600353e+12 1.600360e+12 3359145 285000 NA NA ## avgSpeed maxSpeed avgHr maxHr calories startLongitude startLatitude ## 1 0.0986 0.1122 NA NA 2547.532 NA NA ## 2 0.8160 1.9315 144 176 16345.268 16.31524 48.20915 ## 3 0.8053 1.9557 117 161 8744.572 16.33986 48.34994 ## 4 0.3324 1.2475 155 176 4923.274 16.31587 48.21432 ## 5 0.9629 1.6364 138 163 3083.855 0.00000 0.00000 ## 6 0.1011 0.3524 NA NA 2505.632 NA NA ## aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 1 NA 0.0000 0 ## 2 3.5 0.0000 0 ## 3 2.4 0.0000 0 ## 4 3.0 0.1875 0 ## 5 0.0 0.0000 0 ## 6 NA 0.0000 0 ## elapsedDuration movingDuration anaerobicTrainingEffect deviceId ## 1 3955901 3099653 NA 3968818126 ## 2 16176575 15000179 0.0 3968818126 ## 3 11319866 10356078 0.0 3968818126 ## 4 5538505 5389603 0.2 3968818126 ## 5 3629000 3611000 NA 3825981698 ## 6 3585826 2858951 NA 3968818126 ## minTemperature maxTemperature minElevation maxElevation locationName ## 1 25 26 NA NA &lt;NA&gt; ## 2 19 29 18480 95480 Vienna ## 3 18 29 13980 32200 Korneuburg ## 4 19 27 24920 54400 Vienna ## 5 NA NA 300 3420 City of Westminster ## 6 26 27 NA NA &lt;NA&gt; ## maxVerticalSpeed lapCount endLongitude endLatitude activeSets totalSets ## 1 NA 34 NA NA NA NA ## 2 0.43999939 25 16.25314 48.20399 NA NA ## 3 0.34000092 17 16.38793 48.38009 NA NA ## 4 0.08000183 18 16.31980 48.22264 NA NA ## 5 0.16000004 1 NA NA NA NA ## 6 NA 32 NA NA NA NA ## totalReps purposeful autoCalcCalories favorite pr elevationCorrected ## 1 NA FALSE FALSE FALSE FALSE FALSE ## 2 NA FALSE FALSE FALSE FALSE FALSE ## 3 NA FALSE FALSE FALSE FALSE FALSE ## 4 NA FALSE FALSE FALSE FALSE FALSE ## 5 NA FALSE FALSE FALSE FALSE FALSE ## 6 NA FALSE FALSE FALSE FALSE FALSE ## atpActivity parent maxRunCadence steps avgVerticalOscillation ## 1 FALSE FALSE NA NA NA ## 2 FALSE FALSE NA NA NA ## 3 FALSE FALSE NA NA NA ## 4 FALSE FALSE 104 15620 NA ## 5 FALSE FALSE NA NA NA ## 6 FALSE FALSE NA NA NA ## avgGroundContactTime avgStrideLength vO2MaxValue avgVerticalRatio ## 1 NA NA NA NA ## 2 NA NA 72 NA ## 3 NA NA 71 NA ## 4 NA 115.6389 58 NA ## 5 NA NA NA NA ## 6 NA NA NA NA ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence avgPower ## 1 NA NA NA NA ## 2 NA NA NA 260 ## 3 NA NA NA 202 ## 4 NA 172.375 208 NA ## 5 NA NA NA 212 ## 6 NA NA NA NA ## avgBikeCadence maxBikeCadence strokes normPower avgLeftBalance ## 1 NA NA 1198 NA NA ## 2 83 114 17968 296.0000 49.92 ## 3 82 107 12571 231.0000 49.90 ## 4 NA NA NA NA NA ## 5 91 114 0 218.3049 NA ## 6 NA NA 1152 NA NA ## avgRightBalance max20MinPower trainingStressScore intensityFactor ## 1 NA NA NA NA ## 2 50.08 375.1583 291.4 0.835 ## 3 50.10 242.7692 122.8 0.653 ## 4 NA NA NA NA ## 5 NA 221.0525 NA NA ## 6 NA NA NA NA ## lactateThresholdBpm lactateThresholdSpeed avgStrokes activeLengths avgSwolf ## 1 NA NA 23.0 60 74 ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA 22.6 57 72 ## poolLength avgStrokeDistance avgSwimCadence maxSwimCadence maxFtp workoutId ## 1 5000 217 27 29 NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 5000 221 27 30 NA NA ## decoDive parentId avgVerticalSpeed maxDepth avgDepth surfaceInterval ## 1 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA ## floorsDescended bottomTime ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA You have many options to deal with issues : sep to specify the separator (\\t for tabulation, ';' for semicolon… ) dec the decimal separator encoding the file encoding (special characters from windows/unix systems can be misdetected) colClasses to force one column to be imported in another type than what is detected help(read.table) for more options ! 4.1.2 Excel files You can import excel files (.xls and .xlsx) with the readxl package readxl::read_excel(&quot;Data/Sports/Activities.xlsx&quot;) %&gt;% head() ## # A tibble: 6 x 89 ## activityId uuidMsb uuidLsb name activityType userProfileId timeZoneId ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5570974040 3.6952… -7.098… En p… lap_swimming 1141258 124 ## 2 5566524321 -5.212… -8.140… Vien… cycling 1141258 124 ## 3 5561266034 1.2713… -5.522… Korn… cycling 1141258 124 ## 4 5555881653 -2.923… -4.676… Vien… running 1141258 124 ## 5 5551811953 7.8590… -7.909… Zwif… virtual_ride 1141258 124 ## 6 5551052200 -9.554… -6.866… En p… lap_swimming 1141258 124 ## # … with 82 more variables: beginTimestamp &lt;chr&gt;, eventTypeId &lt;dbl&gt;, ## # rule &lt;chr&gt;, sportType &lt;chr&gt;, startTimeGmt &lt;dbl&gt;, startTimeLocal &lt;chr&gt;, ## # duration &lt;chr&gt;, distance &lt;chr&gt;, elevationGain &lt;chr&gt;, elevationLoss &lt;chr&gt;, ## # avgSpeed &lt;chr&gt;, maxSpeed &lt;chr&gt;, avgHr &lt;chr&gt;, maxHr &lt;chr&gt;, calories &lt;chr&gt;, ## # startLongitude &lt;chr&gt;, startLatitude &lt;chr&gt;, aerobicTrainingEffect &lt;chr&gt;, ## # avgFractionalCadence &lt;chr&gt;, maxFractionalCadence &lt;chr&gt;, ## # elapsedDuration &lt;chr&gt;, movingDuration &lt;chr&gt;, anaerobicTrainingEffect &lt;chr&gt;, ## # deviceId &lt;dbl&gt;, minTemperature &lt;chr&gt;, maxTemperature &lt;chr&gt;, ## # minElevation &lt;chr&gt;, maxElevation &lt;chr&gt;, locationName &lt;chr&gt;, ## # maxVerticalSpeed &lt;chr&gt;, lapCount &lt;dbl&gt;, endLongitude &lt;chr&gt;, ## # endLatitude &lt;chr&gt;, activeSets &lt;chr&gt;, totalSets &lt;chr&gt;, totalReps &lt;chr&gt;, ## # purposeful &lt;chr&gt;, autoCalcCalories &lt;chr&gt;, favorite &lt;chr&gt;, pr &lt;chr&gt;, ## # elevationCorrected &lt;chr&gt;, atpActivity &lt;chr&gt;, parent &lt;chr&gt;, ## # maxRunCadence &lt;chr&gt;, steps &lt;chr&gt;, avgVerticalOscillation &lt;chr&gt;, ## # avgGroundContactTime &lt;chr&gt;, avgStrideLength &lt;chr&gt;, vO2MaxValue &lt;chr&gt;, ## # avgVerticalRatio &lt;chr&gt;, avgGroundContactBalance &lt;chr&gt;, ## # avgDoubleCadence &lt;chr&gt;, maxDoubleCadence &lt;chr&gt;, avgPower &lt;chr&gt;, ## # avgBikeCadence &lt;chr&gt;, maxBikeCadence &lt;chr&gt;, strokes &lt;chr&gt;, normPower &lt;chr&gt;, ## # avgLeftBalance &lt;chr&gt;, avgRightBalance &lt;chr&gt;, max20MinPower &lt;chr&gt;, ## # trainingStressScore &lt;chr&gt;, intensityFactor &lt;chr&gt;, ## # lactateThresholdBpm &lt;chr&gt;, lactateThresholdSpeed &lt;chr&gt;, avgStrokes &lt;chr&gt;, ## # activeLengths &lt;chr&gt;, avgSwolf &lt;chr&gt;, poolLength &lt;chr&gt;, ## # avgStrokeDistance &lt;chr&gt;, avgSwimCadence &lt;chr&gt;, maxSwimCadence &lt;chr&gt;, ## # maxFtp &lt;chr&gt;, workoutId &lt;chr&gt;, decoDive &lt;chr&gt;, parentId &lt;chr&gt;, ## # avgVerticalSpeed &lt;chr&gt;, maxDepth &lt;chr&gt;, avgDepth &lt;chr&gt;, ## # surfaceInterval &lt;chr&gt;, floorsDescended &lt;chr&gt;, bottomTime &lt;chr&gt; Options : sheet to select whioch you want to import range : the “zone” of the sheet you want to import (beginning and ending row/column to be provided) col_types to specify the types of the column if misdetected ?readxl::read_excel for more information 4.1.3 More formats The readr package provides other convenient functions to read the most common (open) formats. With haven, you can also read data from proprietary formats (SPSS, SAS, Stat,…). JSON files can be read with for example rjsonlite and we will use it in an applicaiton example. XML and HTML files can be parsed with the xml2package. 4.1.4 Read from databases / big data This is a huge topic that we will only mention here, but for (almost) each database engine, there is a package available in order to be able to read data from databases General purpose : odbc, RODBC, DBI \\(\\rightarrow\\) you will need to install the DB’s drivers Dedicated : RSQLite, RPostgres, RMariaDB (can be used for mySQL too)… \\(\\rightarrow\\) drivers included With the 3 first package, you can connect to “monolith” databases, as well as to distributed databases. You can find more information on the Rstudio website. Another interesting resources is the dbplyr vignette, that describes how to connect to a database and query it using dplyr’s verbs. In addition, the sparlyr package allows you to interact with a spark cluster (using dplyr syntax) 4.2 The grammar of data manipulation Alert : After this section, pandas will appear much less appealing…. Following the tidy data principles, dplyr implements an actual grammar of data manipulation with verbs and human-readable syntax. 4.2.1 The pipe The first operator to know is the pipe operator, %&gt;% which allows you to redirect the output of a command “to the right” and hence create readable chains of commands. Let’s extract the last 3 characters of “hello world” First solution : create useless objects char &lt;- &quot;hello world&quot; rev_char &lt;- stringi::stri_reverse(char) sub3 &lt;- substr(rev_char,1,3) stringi::stri_reverse(sub3) ## [1] &quot;rld&quot; Second solution : where’s the beginning ???? stringi::stri_reverse(substr(stringi::stri_reverse(&quot;Hello World&quot;),1,3)) ## [1] &quot;rld&quot; Third solution : using the pipe &quot;Hello world&quot; %&gt;% stringi::stri_reverse() %&gt;% substr(1,3) %&gt;% stringi::stri_reverse() ## [1] &quot;rld&quot; Under the hood : the dot represents the result of the previous step and can be placed somewhere else in the next function (rather than the first argument) &quot;Hello world&quot; %&gt;% stringi::stri_reverse(.) %&gt;% substr(.,1,3) %&gt;% stringi::stri_reverse(.) ## [1] &quot;rld&quot; 4.2.2 The verbs of manipulation What do you do with data ? Select columns \\(\\rightarrow\\) select() Filter rows \\(\\rightarrow\\) filter() Create / modify columns \\(\\rightarrow\\) mutate() Compute summaries of the columns \\(\\rightarrow\\) summarise() Do group-wise operations \\(\\rightarrow\\) group_by() Join with other tables \\(\\rightarrow\\) left_join(),right_join(), inner_join(), anti_join(), full_join() I have the verbs, now I can associate them to make a sentence ! All those functions take as first argument a dataframe, which makes it very easy when chaining them with the pipe. read.csv(&quot;Data/Sports/Activities.csv&quot;) %&gt;% select(activityType,avgSpeed,distance,startLongitude,startLatitude,sportType) %&gt;% #select some metrics mutate(distance=distance/100) %&gt;% # distances are in decameter (?) filter(activityType!=&quot;other&quot;) %&gt;% # remove activities &quot;other group_by(activityType) %&gt;% summarise(total_dist=mean(distance)) ## # A tibble: 22 x 2 ## activityType total_dist ## &lt;chr&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 16717. ## 2 cycling 34124. ## 3 cyclocross 41210. ## 4 hiking 10678. ## 5 indoor_cardio NA ## 6 indoor_cycling NA ## 7 indoor_running 2311. ## 8 lap_swimming 3014. ## 9 multi_sport 63873. ## 10 open_water_swimming 2359. ## # … with 12 more rows 4.2.3 Filter : conditions This is the way you write conditions in R : Syntax Condition == Equality test != Different than %in% c(…) Is in this list of values \\(&gt;, &gt;=\\) \\(&lt;, &lt;=\\) Greater/less than ! (x %in% c(…)) Not in the list 4.2.4 Mutate Most of the data manipulation will be done in a mutate statement. This is where you can create additional columns, modify the ones existing. You can do any kind of transformation you want with this one. Depending on the type of the data, here are some additional packages that will help you : lubridate to easily handle date variables forcats to handle factors (categorical variables) stringr (and stringi) to handle strings variables and work with regular expressions ifelse() and case_when() to handle conditional operations require(lubridate) require(stringr) dat &lt;- dat %&gt;% mutate(start_time=as_datetime(startTimeLocal/1000), # create a timestamp date = floor_date(start_time,&quot;day&quot;), # round to the day is_bike=ifelse(activityType %in% c(&quot;cycling&quot;,&quot;virtual_ride&quot;,&quot;indoor_cycling&quot;,&quot;road_biking&quot;,&quot;cyclocross&quot;),T,F), # is it bike or not ? is_run = str_detect(activityType,&quot;running|hicking&quot;), activity_recoded = case_when(is_bike ~ &quot;Bike&quot;, is_run ~ &quot;Run&quot;, str_detect(activityType,&quot;swim&quot;) ~&quot;Swim&quot;, TRUE ~ &quot;Other&quot;)) 4.2.5 Summarize This operation consists in summarizing several rows ofinto one or more synthetic value(s). We will cover the topic more in detail 5 but the most common summary function that you can use are : For continuous variables : average, sum, median, standard deviation, interquartile range (IQR), concentration indexes,… For categorical variables : count, count distinct, concentration indexes,… Simple summary statistics over one numerical variable : dat %&gt;% summarise(total_distance=sum(distance)) # Oups ## total_distance ## 1 NA dat %&gt;% summarise(total_distance=sum(distance,na.rm = T)) ## total_distance ## 1 10693637146 dat %&gt;% summarise(avg_distance=mean(distance,na.rm = T)) ## avg_distance ## 1 1941826 dat %&gt;% summarise(median_distance=median(distance,na.rm = T)) ## median_distance ## 1 1186419 4.2.6 Manipulate several data in the same time With all previous verbs above, you can use the across function to apply the same operation over a bunch of columns that you can select depending a simple enumeration or a condition (on their type or their name). This is a really powerful tool ! Example : we will convert all columns that are identifiers as character variables because the numbers have no meaning dat &lt;- dat %&gt;% mutate(across(c(contains(&quot;Id&quot;),contains(&quot;uuid&quot;)), as.character)) # Other stupid examples dat %&gt;% summarise(across(where(is.numeric), function(xx) sum(xx,na.rm=T))) ## beginTimestamp startTimeGmt startTimeLocal duration distance ## 1 8.021486e+15 8.036783e+15 8.036814e+15 22850917079 10693637146 ## elevationGain elevationLoss avgSpeed maxSpeed avgHr maxHr calories ## 1 163374842 149923907 2186.754 113881.5 538175 637627 20569669 ## startLongitude startLatitude aerobicTrainingEffect avgFractionalCadence ## 1 18707.3 190193.1 7579.3 383.8125 ## maxFractionalCadence elapsedDuration movingDuration anaerobicTrainingEffect ## 1 170.5 11995810258 6623490405 276.1 ## minTemperature maxTemperature minElevation maxElevation maxVerticalSpeed ## 1 46890 65603 35504232 66534189 322.7602 ## lapCount endLongitude endLatitude activeSets totalSets totalReps ## 1 26369 10138.19 39058.89 0 0 0 ## maxRunCadence steps avgVerticalOscillation avgGroundContactTime ## 1 81893 27827620 5202.02 149787.7 ## vO2MaxValue avgVerticalRatio avgGroundContactBalance avgDoubleCadence ## 1 50052 4590.83 29360.86 124685.3 ## maxDoubleCadence avgPower avgBikeCadence maxBikeCadence strokes normPower ## 1 163305 159062 120499 160192 11984172 172253.5 ## avgLeftBalance avgRightBalance max20MinPower trainingStressScore ## 1 29746.24 29853.76 178191.6 146746.6 ## intensityFactor lactateThresholdBpm lactateThresholdSpeed avgStrokes ## 1 640.338 12296 28.8716 36334.99 ## activeLengths avgSwolf poolLength avgStrokeDistance avgSwimCadence ## 1 82178 38881 2028199 113919 21590 ## maxSwimCadence maxFtp avgVerticalSpeed maxDepth avgDepth surfaceInterval ## 1 28979 28273 0 0 0 0 ## floorsDescended bottomTime ## 1 0 0 4.3 Let’s import and wrangle some data ! 4.3.1 The data We will work on the summary data of all past activities, which come in JSON files. So basically, the data is contained in (nested) lists. This is real worl data, it’s super messy and dirty ! You will have to : Import the data Import one of the files using jsonlite Inspect and understand the structure of the list Get all the metrics that are included Figure out how to extract one specific metric for one activity Design a function to extract one metric for all activities contained in the JSON Design a function that will extract all metrics for all activities in the JSON Have a first cleaning of the data : Check the distance/elevation variables ; what do you think ? Check the speed related variables : what do you think ? Check the calories variable and adjust it To help you figuring out, you can check an activity on garmin’s site using this url and change the activity number for the one you are inspecting 4.3.2 One tool you will need : lapply() JSON are lists, and to iterate over list elements, you can either use for loops, which is highly not recommended (R is no good with loops), or use lapply(). This function applies an operation over all elements of a list (or vector) and returns a list containing the result. You can also use sapply() which tries to coerce the result to a vector (if possible) if the expected output is not a list. random_list &lt;- lapply(1:100,function(xx) rnorm(100,xx,xx/5)) str(random_list[1:5]) ## List of 5 ## $ : num [1:100] 1.088 0.585 0.662 1.009 0.897 ... ## $ : num [1:100] 2.08 1.1 2.32 2.14 2.05 ... ## $ : num [1:100] 2.69 5.05 3.18 2.48 3.1 ... ## $ : num [1:100] 4.11 5.61 4.31 4.01 4.59 ... ## $ : num [1:100] 6.28 5.08 5.46 6.97 3.81 ... Attention : be careful with the squared brackets random_list[1] # is a list ## [[1]] ## [1] 1.0877225 0.5851452 0.6620418 1.0088552 0.8966712 0.7728146 0.8424821 ## [8] 0.8421996 0.9534917 0.8250792 0.8536197 0.5801561 0.9478615 0.8904806 ## [15] 0.7775627 0.7667219 1.1631436 0.8313652 0.6544476 0.8968645 1.2619487 ## [22] 1.0792022 0.8329189 1.1315093 1.0625646 0.9984942 0.8425182 0.7974571 ## [29] 1.0747469 0.7644042 0.9904914 0.8879447 0.9015543 0.9520063 0.8518814 ## [36] 1.2758051 0.8732025 0.7284882 1.2260791 0.8097201 1.2786023 0.7538385 ## [43] 1.2766974 0.6145834 0.6047256 0.9439757 0.8979945 1.1262921 0.6468337 ## [50] 1.0199598 0.6490589 1.3579722 0.9792111 0.8604297 1.0297099 0.6828151 ## [57] 1.0161981 1.0751255 1.1408264 0.9717064 0.7145197 0.9238015 0.8487425 ## [64] 0.9686847 1.2110209 1.4824554 0.8353064 0.8540517 1.1400404 0.9074407 ## [71] 0.7995750 0.9841000 1.0853213 1.3207973 1.2986585 0.8741733 0.9802781 ## [78] 1.1310078 1.0739595 1.3361277 1.2769076 0.7837746 1.0228181 0.5874196 ## [85] 1.2734078 0.9600373 0.7616597 1.0626225 0.9101277 1.1358055 0.8980311 ## [92] 1.1669732 1.1595824 1.2163830 0.7706699 0.8562499 0.9541199 0.6634264 ## [99] 1.0624170 0.9793731 random_list[[1]] # is a vector ## [1] 1.0877225 0.5851452 0.6620418 1.0088552 0.8966712 0.7728146 0.8424821 ## [8] 0.8421996 0.9534917 0.8250792 0.8536197 0.5801561 0.9478615 0.8904806 ## [15] 0.7775627 0.7667219 1.1631436 0.8313652 0.6544476 0.8968645 1.2619487 ## [22] 1.0792022 0.8329189 1.1315093 1.0625646 0.9984942 0.8425182 0.7974571 ## [29] 1.0747469 0.7644042 0.9904914 0.8879447 0.9015543 0.9520063 0.8518814 ## [36] 1.2758051 0.8732025 0.7284882 1.2260791 0.8097201 1.2786023 0.7538385 ## [43] 1.2766974 0.6145834 0.6047256 0.9439757 0.8979945 1.1262921 0.6468337 ## [50] 1.0199598 0.6490589 1.3579722 0.9792111 0.8604297 1.0297099 0.6828151 ## [57] 1.0161981 1.0751255 1.1408264 0.9717064 0.7145197 0.9238015 0.8487425 ## [64] 0.9686847 1.2110209 1.4824554 0.8353064 0.8540517 1.1400404 0.9074407 ## [71] 0.7995750 0.9841000 1.0853213 1.3207973 1.2986585 0.8741733 0.9802781 ## [78] 1.1310078 1.0739595 1.3361277 1.2769076 0.7837746 1.0228181 0.5874196 ## [85] 1.2734078 0.9600373 0.7616597 1.0626225 0.9101277 1.1358055 0.8980311 ## [92] 1.1669732 1.1595824 1.2163830 0.7706699 0.8562499 0.9541199 0.6634264 ## [99] 1.0624170 0.9793731 4.4 The split-apply-combine operation 4.5 Tidy your data gather/spread "],
["stats.html", "Chapter 5 Statistics 5.1 Definitions 5.2 Univariate statistics 5.3 Bivariate statistics 5.4 Statistical inference", " Chapter 5 Statistics This session aims to give a practical guide to explore a dataset you’ve never seen before and to understand some of the key statistical concepts. You will learn to describe each variable of a dataset and assess the strength of the relationship between two variables whatever their types may be. For that, we’ll see how to visually explore a dataset and to quantify what the graphics show. We will also give an overview of what statistical inference is and what it can be used for. This section covers the following topics : Definitions Descriptive statistics Univariate statistics Bivariate statistics Statistical inference : The statistical model Main theorems to be aware of Introduction to statistical tests 5.1 Definitions 5.1.1 Terminology A data set can be viewed in two different manners : A set of rows, or statistical individuals, aka observations (or instances in the galaxy of machine learning). This can be anything A set of columns, or variables that describe the individuals It is crucial to have a good understanding of what the statistical indvidual is, and that can be challenging ! Some examples : head(dat) ## activityId uuidMsb uuidLsb name ## 1 5570974040 3695223521635878400 -7098506714510231552 En piscine ## 2 5566524321 -5212790351453402112 -8140622953684101120 Vienna Cyclisme ## 3 5561266034 1271355725617447936 -5522843681386392576 Korneuburg Cyclisme ## 4 5555881653 -2923937867469140992 -4676832321872288768 Vienna Course ## 5 5551811953 7859042982901073920 -7909531516612413440 Zwift - London ## 6 5551052200 -955408594416025216 -6866434522826685440 En piscine ## activityType userProfileId timeZoneId beginTimestamp eventTypeId rule ## 1 lap_swimming 1141258 124 1.600700e+12 9 public ## 2 cycling 1141258 124 1.600602e+12 9 public ## 3 cycling 1141258 124 1.600523e+12 9 public ## 4 running 1141258 124 1.600439e+12 9 public ## 5 virtual_ride 1141258 124 1.600362e+12 9 public ## 6 lap_swimming 1141258 124 1.600353e+12 9 public ## sportType startTimeGmt startTimeLocal duration distance elevationGain ## 1 GENERIC 1.600700e+12 1.600708e+12 3640221 300000 NA ## 2 CYCLING 1.600602e+12 1.600609e+12 15064570 12293220 200100 ## 3 CYCLING 1.600523e+12 1.600530e+12 10382433 8360927 74500 ## 4 RUNNING 1.600439e+12 1.600447e+12 5393830 1792890 42600 ## 5 GENERIC 1.600362e+12 1.600369e+12 3626825 3492347 16000 ## 6 GENERIC 1.600353e+12 1.600360e+12 3359145 285000 NA ## elevationLoss avgSpeed maxSpeed avgHr maxHr calories startLongitude ## 1 NA 0.0986 0.1122 NA NA 2547.532 NA ## 2 196800 0.8160 1.9315 144 176 16345.268 16.31524 ## 3 74200 0.8053 1.9557 117 161 8744.572 16.33986 ## 4 42500 0.3324 1.2475 155 176 4923.274 16.31587 ## 5 0 0.9629 1.6364 138 163 3083.855 0.00000 ## 6 NA 0.1011 0.3524 NA NA 2505.632 NA ## startLatitude aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 1 NA NA 0.0000 0 ## 2 48.20915 3.5 0.0000 0 ## 3 48.34994 2.4 0.0000 0 ## 4 48.21432 3.0 0.1875 0 ## 5 0.00000 0.0 0.0000 0 ## 6 NA NA 0.0000 0 ## elapsedDuration movingDuration anaerobicTrainingEffect deviceId ## 1 3955901 3099653 NA 3968818126 ## 2 16176575 15000179 0.0 3968818126 ## 3 11319866 10356078 0.0 3968818126 ## 4 5538505 5389603 0.2 3968818126 ## 5 3629000 3611000 NA 3825981698 ## 6 3585826 2858951 NA 3968818126 ## minTemperature maxTemperature minElevation maxElevation locationName ## 1 25 26 NA NA &lt;NA&gt; ## 2 19 29 18480 95480 Vienna ## 3 18 29 13980 32200 Korneuburg ## 4 19 27 24920 54400 Vienna ## 5 NA NA 300 3420 City of Westminster ## 6 26 27 NA NA &lt;NA&gt; ## maxVerticalSpeed lapCount endLongitude endLatitude activeSets totalSets ## 1 NA 34 NA NA NA NA ## 2 0.43999939 25 16.25314 48.20399 NA NA ## 3 0.34000092 17 16.38793 48.38009 NA NA ## 4 0.08000183 18 16.31980 48.22264 NA NA ## 5 0.16000004 1 NA NA NA NA ## 6 NA 32 NA NA NA NA ## totalReps purposeful autoCalcCalories favorite pr elevationCorrected ## 1 NA FALSE FALSE FALSE FALSE FALSE ## 2 NA FALSE FALSE FALSE FALSE FALSE ## 3 NA FALSE FALSE FALSE FALSE FALSE ## 4 NA FALSE FALSE FALSE FALSE FALSE ## 5 NA FALSE FALSE FALSE FALSE FALSE ## 6 NA FALSE FALSE FALSE FALSE FALSE ## atpActivity parent maxRunCadence steps avgVerticalOscillation ## 1 FALSE FALSE NA NA NA ## 2 FALSE FALSE NA NA NA ## 3 FALSE FALSE NA NA NA ## 4 FALSE FALSE 104 15620 NA ## 5 FALSE FALSE NA NA NA ## 6 FALSE FALSE NA NA NA ## avgGroundContactTime avgStrideLength vO2MaxValue avgVerticalRatio ## 1 NA &lt;NA&gt; NA NA ## 2 NA &lt;NA&gt; 72 NA ## 3 NA &lt;NA&gt; 71 NA ## 4 NA 115.638917703599 58 NA ## 5 NA &lt;NA&gt; NA NA ## 6 NA &lt;NA&gt; NA NA ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence avgPower ## 1 NA NA NA NA ## 2 NA NA NA 260 ## 3 NA NA NA 202 ## 4 NA 172.375 208 NA ## 5 NA NA NA 212 ## 6 NA NA NA NA ## avgBikeCadence maxBikeCadence strokes normPower avgLeftBalance ## 1 NA NA 1198 NA NA ## 2 83 114 17968 296.0000 49.92 ## 3 82 107 12571 231.0000 49.90 ## 4 NA NA NA NA NA ## 5 91 114 0 218.3049 NA ## 6 NA NA 1152 NA NA ## avgRightBalance max20MinPower trainingStressScore intensityFactor ## 1 NA NA NA NA ## 2 50.08 375.1583 291.4 0.835 ## 3 50.10 242.7692 122.8 0.653 ## 4 NA NA NA NA ## 5 NA 221.0525 NA NA ## 6 NA NA NA NA ## lactateThresholdBpm lactateThresholdSpeed avgStrokes activeLengths avgSwolf ## 1 NA NA 23.0 60 74 ## 2 NA NA NA NA NA ## 3 NA NA NA NA NA ## 4 NA NA NA NA NA ## 5 NA NA NA NA NA ## 6 NA NA 22.6 57 72 ## poolLength avgStrokeDistance avgSwimCadence maxSwimCadence maxFtp workoutId ## 1 5000 217 27 29 NA &lt;NA&gt; ## 2 NA NA NA NA NA &lt;NA&gt; ## 3 NA NA NA NA NA &lt;NA&gt; ## 4 NA NA NA NA NA &lt;NA&gt; ## 5 NA NA NA NA NA &lt;NA&gt; ## 6 5000 221 27 30 NA &lt;NA&gt; ## decoDive parentId avgVerticalSpeed maxDepth avgDepth surfaceInterval ## 1 NA &lt;NA&gt; NA NA NA NA ## 2 NA &lt;NA&gt; NA NA NA NA ## 3 NA &lt;NA&gt; NA NA NA NA ## 4 NA &lt;NA&gt; NA NA NA NA ## 5 NA &lt;NA&gt; NA NA NA NA ## 6 NA &lt;NA&gt; NA NA NA NA ## floorsDescended bottomTime start_time date is_bike is_run ## 1 NA NA 2020-09-21 17:00:51 2020-09-21 FALSE FALSE ## 2 NA NA 2020-09-20 13:37:03 2020-09-20 TRUE FALSE ## 3 NA NA 2020-09-19 15:38:41 2020-09-19 TRUE FALSE ## 4 NA NA 2020-09-18 16:28:37 2020-09-18 FALSE TRUE ## 5 NA NA 2020-09-17 18:55:06 2020-09-17 TRUE FALSE ## 6 NA NA 2020-09-17 16:33:09 2020-09-17 FALSE FALSE ## activity_recoded ## 1 Swim ## 2 Bike ## 3 Bike ## 4 Run ## 5 Bike ## 6 Swim group_by(dat,activityType) %&gt;% summarise(total_dist=sum(distance,na.rm=T),avg_speed=mean(avgSpeed,na.rm=T),avg_power=mean(avgPower,na.rm = T), .groups=&quot;keep&quot;) %&gt;% head() ## # A tibble: 6 x 4 ## # Groups: activityType [6] ## activityType total_dist avg_speed avg_power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 63523673. 0.350 NaN ## 2 cycling 7418503692. 0.633 253. ## 3 cyclocross 4121029 0.512 NaN ## 4 hiking 7474908. 0.110 NaN ## 5 indoor_cardio 0 0 NaN ## 6 indoor_cycling 59231795 0.0431 225. group_by(dat,date) %&gt;% summarise(total_dist=sum(distance,na.rm=T),avg_speed=mean(avgSpeed,na.rm=T),avg_power=mean(avgPower,na.rm = T), .groups=&quot;keep&quot;) %&gt;% head() ## # A tibble: 6 x 4 ## # Groups: date [6] ## date total_dist avg_speed avg_power ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008-05-27 00:00:00 942603 0.582 NaN ## 2 2008-11-25 00:00:00 924717 0.648 NaN ## 3 2008-11-26 00:00:00 1994601 0.353 NaN ## 4 2008-11-27 00:00:00 2130961 0.615 NaN ## 5 2008-11-28 00:00:00 1061117 0.366 NaN ## 6 2008-11-29 00:00:00 20828 0.192 NaN 5.1.2 Types of variables The way we analyse variables depends on their type : Numerical variables : Continuous : income, revenue \\(\\in \\mathbb{R} , \\mathbb{R}^+\\) Discrete : number of person per household \\(\\in \\mathbb{Z} , \\mathbb{N}\\) Categorical variables : Ordered : small, medium, large Unordered : male, female 5.2 Univariate statistics 5.2.1 Numerical variables 5.2.1.1 Distribution The distribution of a variable quantifies the number of individuals how have a certain value of the variable. We can visualize the distribution either with histograms or density plot, which are the “empirical counterparts” of the probability density function. ggplot(dat,aes(avgPower)) + geom_histogram() + theme_minimal() ggplot(dat,aes(avgPower)) + geom_density() + theme_minimal() 5.2.1.2 Descriptive statistics We typically want to measure what the “average” value is, along with “how diverse is my population”. For that, we can use either sum-based statistics (mean, standard deviation) or quantiles. Quantile-based statistics are said to be robust because much less sensitive to outliers. But they are more computationally expensive. stats &lt;- c(quantile(dat$avgPower,1:3/4,na.rm = T),mean(dat$avgPower,na.rm = T)) ggplot(dat,aes(avgPower)) + geom_histogram() + geom_vline(xintercept = stats,color=&quot;red&quot;) + annotate(geom = &quot;text&quot;,x=stats,y=c(50,40,50,100),label=c(&quot;Q1&quot;,&quot;Q2=median&quot;,&quot;Q3&quot;,&quot;mean&quot;),color=&quot;red&quot;) + geom_segment(aes(x=200,y=25,xend=300,yend=25),color=&quot;blue&quot;) + annotate(geom=&quot;text&quot;,x=300,y=30,label=&quot;dispersion&quot;,color=&quot;blue&quot;) + theme_minimal() 5.2.1.2.1 Central tendency Central tendency statistics allow you to have an idea of the order of magnitude of the attribute you are interested in, over the population. summary(dat$avgPower) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 224.0 239.0 240.6 261.0 347.0 4853 quantile(dat$avgPower,probs=0:10/10,na.rm = T) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0 212 221 226 230 239 246 258 265 276 347 5.2.1.2.2 Dispersion Dispersion describes how heterogenous our population is. It can be measured with various measurements (not exhaustive here) sd(dat$avgPower,na.rm = T) # standard deviation ## [1] 29.83959 IQR(dat$avgPower,na.rm = T) # interquartile range ## [1] 37 sd(dat$avgPower,na.rm = T)/mean(dat$avgPower,na.rm = T) # coefficient of variation ## [1] 0.1240017 How to read it : The average deviation to the average power is 29 watts The age difference between the rides in the 25% “less powerful” rides and the 25% “most powerful” rides is 37 watts The average deviation to the average power is 12% of the average power The latter allows to compare dispersion between variables that have different units 5.2.1.3 Dealing with various shapes The traditional example of a distribution is the gaussian distribution fake &lt;- data.frame(xx=rnorm(100000,100,10)) ggplot(fake,aes(xx)) + geom_histogram() + labs(x=&quot;Random variable&quot;)+ theme_minimal() In this case, we have a very interesting property : symmetry, which makes mean and median very close. If the coefficient of variation is not too high, the tail is pretty short. In real life,it (almost) never happens. Therefore, to understand what happens, you can check : How different are mean and median Does a log transformation make the distribution “look better” Is it symmetric \\(\\rightarrow\\) skewness Is flat no not \\(\\rightarrow\\) kurtosis Is the distribution highly concentrated (few individuals get almost the whole cake) \\(\\rightarrow\\) concentration indexes (Gini, enthropy, Herfindahl…). You can check the package ineq Are there outliers (which generates a long tail) \\(\\rightarrow\\) outlier detection (vast field…). You can start with the previous Flat or not flat ? data.frame(xx=rnorm(10000),yy=rnorm(10000,0,5)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(value,color=name)) + geom_density()+ theme_minimal() ggplot(dat,aes(distance)) + geom_histogram() + theme_minimal() ggplot(dat,aes(distance)) + geom_histogram() + scale_x_log10() + theme_minimal() 5.2.1.4 Exercises : What can you tell about the distance variable ? - Draw the distribution of this variable. How much is the maximum distance of the 20% shortest activities ; the minimum distance of the 5% longest activities ? - What unit do you think it is ? Did you check the maximum value ? - Is there more dispersion in the distance or the average power ? using the facet_wrap function of ggplot2, compare the distributions of distance and avgPower. - I want to group activities in 5 categories based on the distance. This operation is called discretization (very useful for choropleth maps). Search for available methods, and apply some of them. Which one is best suited to this variable ? Which one should you avoid ? 5.2.2 Categorical variables 5.2.2.1 Working with factors Factors are an optimized way to store categorical variables (encoded in integers). The distinct categories are stored in the level attribute which you can interact with. as.factor(dat$activityType) %&gt;% levels() ## [1] &quot;cross_country_skiing_ws&quot; &quot;cycling&quot; ## [3] &quot;cyclocross&quot; &quot;hiking&quot; ## [5] &quot;indoor_cardio&quot; &quot;indoor_cycling&quot; ## [7] &quot;indoor_running&quot; &quot;lap_swimming&quot; ## [9] &quot;multi_sport&quot; &quot;open_water_swimming&quot; ## [11] &quot;other&quot; &quot;road_biking&quot; ## [13] &quot;running&quot; &quot;street_running&quot; ## [15] &quot;strength_training&quot; &quot;swimming&quot; ## [17] &quot;swimToBikeTransition&quot; &quot;trail_running&quot; ## [19] &quot;transition&quot; &quot;treadmill_running&quot; ## [21] &quot;uncategorized&quot; &quot;virtual_ride&quot; ## [23] &quot;walking&quot; as.factor(dat$activityType) %&gt;% str() ## Factor w/ 23 levels &quot;cross_country_skiing_ws&quot;,..: 8 2 2 13 22 8 2 13 2 8 ... For more functionalities you can use the forcats package which provides convenient tools (eg to recode the variable) 5.2.2.2 Barcharts The barchart (which IS NOT a histogram) is the most common representation for categorical variables. You can also use the pie chart (but it requires to hack a little ggplot). Pie charts are despised by the majority of statisticians but it can be adapted if the sizes really differ. Some material to make your own opinion : Why it’s bad Defense ggplot(dat,aes(activityType)) + geom_bar() + coord_flip() + theme_minimal() ggplot(dat,aes(x=&quot;&quot;,fill=activityType)) + geom_bar(width=1) + coord_polar(&quot;y&quot;,start=0) + theme_void() 5.2.2.3 Contingency tables After visualizing, how can we measure the number of cases and the percent in each category ? table(dat$activityType) ## ## cross_country_skiing_ws cycling cyclocross ## 38 2174 1 ## hiking indoor_cardio indoor_cycling ## 7 32 331 ## indoor_running lap_swimming multi_sport ## 13 852 52 ## open_water_swimming other road_biking ## 69 22 2 ## running street_running strength_training ## 1728 10 4 ## swimming swimToBikeTransition trail_running ## 4 1 1 ## transition treadmill_running uncategorized ## 63 4 55 ## virtual_ride walking ## 49 2 table(dat$activityType) %&gt;% prop.table()*100 ## ## cross_country_skiing_ws cycling cyclocross ## 0.68915488 39.42691331 0.01813565 ## hiking indoor_cardio indoor_cycling ## 0.12694958 0.58034095 6.00290170 ## indoor_running lap_swimming multi_sport ## 0.23576351 15.45157780 0.94305404 ## open_water_swimming other road_biking ## 1.25136017 0.39898440 0.03627131 ## running street_running strength_training ## 31.33841132 0.18135655 0.07254262 ## swimming swimToBikeTransition trail_running ## 0.07254262 0.01813565 0.01813565 ## transition treadmill_running uncategorized ## 1.14254625 0.07254262 0.99746101 ## virtual_ride walking ## 0.88864708 0.03627131 Another solution is to use what you’ve learned in the previous section (@ref()) : aggregation ! group_by(dat,activityType) %&gt;% summarise(number=n(),proportion=n()/nrow(dat)) ## # A tibble: 23 x 3 ## activityType number proportion ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 38 0.00689 ## 2 cycling 2174 0.394 ## 3 cyclocross 1 0.000181 ## 4 hiking 7 0.00127 ## 5 indoor_cardio 32 0.00580 ## 6 indoor_cycling 331 0.0600 ## 7 indoor_running 13 0.00236 ## 8 lap_swimming 852 0.155 ## 9 multi_sport 52 0.00943 ## 10 open_water_swimming 69 0.0125 ## # … with 13 more rows 5.3 Bivariate statistics In this section, we see how to represent the relationship between two variables and measure it 5.3.1 2 continuous variables 5.3.1.1 Graphical exploration To visualize the relationship between two numerical variables, we can use the scatter plot. Don’t forget that the log function can help you identify non linear relationships since \\(log(a \\cdot x^b) = log(a) + b \\cdot log(x)\\) ggplot(dat,aes(distance,avgPower)) + geom_jitter() + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Raw variables&quot;) + scale_x_continuous(labels = scales::comma)+ theme_minimal() ggplot(dat,aes(distance,avgPower)) + geom_jitter() + scale_x_log10(labels = scales::comma) + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Distance in log scale&quot;) + theme_minimal() We can pimp up the graphics a bit to visualize the correlation ggplot(dat,aes(distance/1E5,avgPower)) + geom_jitter() + scale_x_log10(labels = scales::comma) + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Usage in log scale&quot;)+ geom_smooth(method=&quot;lm&quot;) + theme_minimal() We can see here that there is a positive relationship between distance and data usage and that this relationship has an exponential shape, meaning that the usage increases A LOT when the age drops. 5.3.1.2 Quantifying the relationship : correlations To quantify this relationship, you can use the coefficients of correlation. There are 3 main coefficients : Pearson (the most famous and used), Kendall and Spearman. The latter can handle non-linear functional dependencies (ranks correlation) ; this is (roughly) equivalent to computing the coefficients on the log-transformed variables. print(&quot;Pearson coeff&quot;) ## [1] &quot;Pearson coeff&quot; cor(dat$distance,dat$avgPower,method=&quot;pearson&quot;) ## [1] NA print(&quot;Pearson coeff, NAs removed&quot;) ## [1] &quot;Pearson coeff, NAs removed&quot; cor(dat$distance,dat$avgPower,method=&quot;pearson&quot;,use = &quot;complete.obs&quot;) ## [1] 0.5437859 print(&quot;Spearman coeff&quot;) ## [1] &quot;Spearman coeff&quot; cor(dat$distance,dat$avgPower,method=&quot;spearman&quot;,use = &quot;complete.obs&quot;) ## [1] 0.6316721 # Why is R a beautiful language ? Do it at once, without loops (loops are evil) print(&quot;all coeffs&quot;) ## [1] &quot;all coeffs&quot; sapply(c(&quot;pearson&quot;,&quot;spearman&quot;,&quot;kendall&quot;),function(xx) cor(dat$distance,dat$avgPower,method=xx,use = &quot;complete.obs&quot;)) ## pearson spearman kendall ## 0.5437859 0.6316721 0.4452121 More info about correlation coefficients Should there be a complex relationship (eg sine), the graphical exploration is mandatory ! 5.3.2 2 categorical variables For this part, I will create a discrete variable out of the distance variable (see previous exercises) to use it as second qualitative variable (the other are not really meaningful) dat &lt;- mutate(dat,qual_distance=as.character(cut(distance, quantile(distance,probs = 0:5/5,na.rm=T), include.lowest = T, labels=c(&quot;Very short&quot;,&quot;Short&quot;, &quot;Average&quot;,&quot;Long&quot;,&quot;Very long&quot;))), qual_avgHr=as.character(cut(avgHr,quantile(avgHr,0:3/5,na.rm = T), include.lowest = T, labels=c(&quot;Low intensity&quot;,&quot;Average intensity&quot;, &quot;High intensity&quot;))), qual_distance=ifelse(is.na(qual_distance),&quot;Very short&quot;,qual_distance)) Try different layouts with you barcharts ! 5.3.2.1 Barcharts ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;stack&quot;) + theme_minimal() ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;dodge&quot;) + theme_minimal() ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;fill&quot;) + scale_y_continuous(labels = scales::percent) + theme_minimal() You get really different insights depending on the representation you chose ! 5.3.2.2 Contingency tables table(dat$activity_recoded,dat$qual_distance) ## ## Average Long Short Very long Very short ## Bike 530 297 186 986 558 ## Other 48 39 34 43 111 ## Run 523 764 311 70 88 ## Swim 0 1 554 3 368 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table()*100 ## ## Average Long Short Very long Very short ## Bike 9.61189699 5.38628945 3.37323177 17.88175553 10.11969532 ## Other 0.87051143 0.70729053 0.61661226 0.77983315 2.01305767 ## Run 9.48494741 13.85564019 5.64018861 1.26949583 1.59593761 ## Swim 0.00000000 0.01813565 10.04715270 0.05440696 6.67392093 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table(margin = 1)*100 ## ## Average Long Short Very long Very short ## Bike 20.7274149 11.6151740 7.2741494 38.5608135 21.8224482 ## Other 17.4545455 14.1818182 12.3636364 15.6363636 40.3636364 ## Run 29.7835991 43.5079727 17.7107062 3.9863326 5.0113895 ## Swim 0.0000000 0.1079914 59.8272138 0.3239741 39.7408207 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table(margin = 2)*100 ## ## Average Long Short Very long Very short ## Bike 48.13805631 26.97547684 17.14285714 89.47368421 49.60000000 ## Other 4.35967302 3.54223433 3.13364055 3.90199637 9.86666667 ## Run 47.50227066 69.39146231 28.66359447 6.35208711 7.82222222 ## Swim 0.00000000 0.09082652 51.05990783 0.27223230 32.71111111 5.3.2.3 Quantifying relationships : \\(\\chi^2\\), Cramer’s V The Chi-square (\\(\\chi^2\\)) statistics is used to measure the distance between the actual distribution of cases among categories of both variables and the distribution if the variables were independant. The higher the X-squared, the higher the divergence with independance, meaning that the variables are likely linked (correlation does not apply to categorical variables). The p-value indicates whether this relationship is statistically significant or not. We will see this in more details in the last chapter (Inference). More info and detailed way to compute the value : this website 5.3.2.4 Extreme examples : Let’s assume we want to assess the relationship between gender and churn. Further assumption, we have 100 customers, 50 males, 50 females on the one hand, 50 churners, 50 non churners on the other hand. If the variables are independent, the cross table would look that way : ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 0, df = 12, p-value = 1 In the opposite situation (full dependency), the contingency table would look like that : ## Short Very long Long Average ## Swim 25 0 0 0 ## Bike 0 25 0 0 ## Run 0 0 25 0 ## Other 0 0 0 25 The \\(\\chi^2\\) statistic measures the “distance” between reality and the first case (independence) Note : if some cells of the contingency table have less than 5 cases, the statistic is not reliable (you’ll get a message in this case) The chi-square suffers 2 main drawbacks : its value depends on the number of observations and the total number of categories \\(\\Rightarrow\\) one cannot compare the \\(\\chi^2\\) values for 2 different tables that have different numbers of underlying observations and number of categories. To deal with that, you can use Cramer’s V, which is a (kind of) normalized \\(\\chi^2\\). You can use the function built in the lsr package. Cramer’s V \\(\\in [0,1]\\) and the higher it is, the more intense the link between both variables. # install.packages(&quot;lsr&quot;) # If not installed table(dat$activity_recoded,dat$qual_distance) %&gt;% lsr::cramersV() ## [1] 0.4454809 Let’s check with our 2 extreme examples : lsr::cramersV(ex_dep[,-5]) ## [1] 1 lsr::cramersV(ex_indep) ## [1] 0 In practice, it is very rare to get high values ; a rule of thumb is that a value around 0.2-0.3 is already “decent”. The \\(\\chi^2\\) p-value (if under 0.05) shows that there is a relationship ; Cramer’s V allows to compare between two tables. 5.3.3 1 continuous, 1 categorical variable In this part, we see how to deal with 2 variables that have different types. The goal remains the same : getting insights about the relationship between those 2 variables and quantify the strength of the link. We will try to assess if there is a connection between the distance and the discipline. 5.3.3.1 Boxplots, violin plots Boxplots are a simple, effective and compact representation of a variable’s distribution. It relies on quantiles. Vilin plots allows to see the ful distribution of both variables # Compute bounds of the boxplot # bounds &lt;- group_by(dat,activity_recoded) %&gt;% # summarise(q1=quantile(distance,.25,na.rm = T), # q2=quantile(distance,.50,na.rm = T), # q3=quantile(distance,.75,na.rm = T), # lower_bound=q1-1.5*IQR(distance,na.rm = T), # upper_bound=q3+1.5*IQR(distance,na.rm = T)) %&gt;% # pivot_longer(-activity_recoded) ggplot(dat,aes(activity_recoded,distance/1E5)) + geom_boxplot() + theme_minimal() # geom_point(data = bounds,aes(activity_recoded,value,color=name)) ggplot(dat,aes(activity_recoded,distance/1E5)) + geom_violin()+ theme_minimal() + scale_y_log10(label=scales::comma) It looks like bike activities are longer than the others ! Big surprise ! 5.3.3.2 Quantifying relationship intensity : \\(\\eta^2\\) The graphics indicate that there is a relationship between activity type and distance (if not, boxplots would have the same shape for all groups). We can assess the strength of the connection with the \\(\\eta^2\\) statistics. Using the decomposition of the variance formula \\(SS_{total} = SS_{between} + SS_{within}\\), \\(\\eta^2\\) is defined as \\(\\eta^2 = \\dfrac{SS_{between}}{SS_{total}} \\in [0,1]\\) 5.3.3.3 Extreme examples : If the variables are independent, the boxplots should look like this (almost no difference in the distributions) : If they are fully “correlated”, the activity variable would explain all variance in the data set : In this case, we see that all the variance lies between the subgroups : there is no dispersion within the groups. Note : In practice, the previous situation will of course never happen, and a categorical variable can’t carry by itself a lot of variance (since the number of possible values are de facto limited). This is also the \\(R^2\\) of the 1-factor ANOVA regression of distance explained by activty type, anova &lt;- aov(distance~activity_recoded,data=dat) print(&quot;Variance decomposition&quot;) ## [1] &quot;Variance decomposition&quot; summary(anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## activity_recoded 3 6.106e+15 2.035e+15 230.6 &lt;2e-16 *** ## Residuals 5503 4.858e+16 8.828e+12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 7 observations deleted due to missingness print(&quot;eta squared&quot;) ## [1] &quot;eta squared&quot; lsr::etaSquared(anova) ## eta.sq eta.sq.part ## activity_recoded 0.1116542 0.1116542 print(&quot;Alternatively&quot;) ## [1] &quot;Alternatively&quot; lm(distance~activity_recoded,data=dat) %&gt;% summary() ## ## Call: ## lm(formula = distance ~ activity_recoded, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2998157 -1747410 -123419 136538 113607973 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2998157 58768 51.017 &lt; 2e-16 *** ## activity_recodedOther -1191531 189496 -6.288 3.47e-10 *** ## activity_recodedRun -1706130 92138 -18.517 &lt; 2e-16 *** ## activity_recodedSwim -2702238 113960 -23.712 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2971000 on 5503 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.1117, Adjusted R-squared: 0.1112 ## F-statistic: 230.6 on 3 and 5503 DF, p-value: &lt; 2.2e-16 In this case, 11.2% of the age variance is explained by the difference in activity types ; it is very high. 5.3.4 Exercises Explore the distribution of the average speed. What can you say about it ? Explore the correlation between average speed and average power For all the the categorical variables, get the frequent category (with table AND dplyr/tidyr) Very important : For the next parts, we will remove the extreme observation that is clearly an error dat_clean &lt;- filter(dat,!(activityId %in% c(407226313,2321338)) &amp; year(date)&gt;=2012) 5.4 Statistical inference 5.4.1 The statistical model We want to measure a characteristic in the general population, let’s say the usage of fixed data and let’s denote it by U. The fundamental assumption of the statistical model is that there is an underlying data-genereting process, which means that U is distributed according to a certain probability distribution. The goal of the statistician is to find which distribution it is, and estimate its parameters. The big problem is that it is impossible to observe U on the whole population (even with a census), and a dataset is only a sample of the general population (which does not really exists). The question is then : how can we estimate the parameters of the true distribution ? \\(\\Rightarrow\\) There is a difference between the sample mean and the population mean (noted \\(\\mu\\)). As a matter of fact the sample mean is an estimator of the population mean. The value of an estimator (often noted \\(\\hat{\\theta}\\)) is a random variable (it depends on the sample), meaning this is not a single deterministic value, but has a probability distribution. Therefore it has a expectation and a variance An estimator is said to be biased if \\(\\mathbb{E}(\\hat{\\theta}) \\neq \\mu\\) ; it is said to be efficient if its variance is minimal. 5.4.2 Two fundamental theorems Eventough we cannot observe the true parameter and that the sample mean is an estimator (hence a random variable), 2 theorems save the game : 5.4.3 The law of large numbers \\[\\bar{U} = \\dfrac{1}{n} \\sum_{i=1}^n U_i \\xrightarrow[n \\to +\\infty]{a.s.} \\mu\\] In other words, when the sample size n is big enough, the sample mean converges to the population mean \\(\\rightarrow\\) We can estimate this parameter with a simple mean without bias. 5.4.3.1 The central limit theorem (CLT) Probably the most important theorem in statistics, valid whatever the true distribution is \\[\\sqrt{n} \\cdot \\bar{U} \\xrightarrow[n \\to +\\infty]{p} \\mathcal{N} (\\mu,\\sigma^2)\\] Meaning that the sample mean converges in probability to a normal distribution with population parameters at “speed” \\(\\sqrt{n}\\). This equivalent to : \\[ \\bar{U} - \\mu \\xrightarrow[n \\to +\\infty]{p} \\mathcal{N} (0,\\frac{\\sigma^2}{n})\\] Meaning that : I can quantify “how far” my sample mean is from the true value The larger the sample size, the smaller the average deviation to the true value \\(\\rightarrow\\) the variance of my estimator reduces when the sample size increases. Take away : The sample mean is an estimator of the true value of an underlying “true” mean. The estimator’s value depends on the sample I have This estimator (any estimator) has a variance that I could measure if I had several samples to compute several sample means Probality theory gives us tools to estimate the bias and the variance of an estimator Bias-variance trade-off :\\(\\mathbb{E}((U-\\bar{U})^2)=\\mathbb{E}^2(U-\\bar{U}) + \\mathbb{V}(\\bar{U})\\), in other words : \\(MSE_{\\bar{U}} = bias^2 + \\mathbb{V}(\\bar{U})\\) \\(\\rightarrow\\) see you during ML course ;-) Illustration of the biais-variance trade-off. Let’s assume the true average of the usage is 10, what do you prefer over the following scenarios ? Let’s simulate two distributions (let’s say it is the distribution of 2 different estimators) : One with mean 10 and variance 4 \\(\\rightarrow\\) unbiased The second with mean 10.5 and variance 1 \\(\\rightarrow\\) biased but with low variance In the first case, the estimator is unbiased, but with a higher variance than the second : if we go for it, we take the chance to have an estimate (depending on our sample) of eg 15 or 5, which is almost unlikely to happen with the second estimator, although this second is not centered on the true value. It’s up to you to decide, but you generally can’t have both an unbiased and very precise estimator… 5.4.4 Statistical tests 5.4.4.1 Introductory example Knowing the theoretical probability distribution of our estimator, we can assess the likelihood of an hypothesis. For the example, let’s take the hypothesis (\\(\\mathcal{H_0}\\)) that the true mean is 10 and stardard deviation is \\(2\\sqrt{n}\\). If this hypothesis is true, thanks to the CTL, the distribution of \\(\\bar{U}\\) would be the following : Now, I can compute my sample mean and check its value against the hypothetical distribution : Obviously my actual value doesn’t fit with my hypothesis : the probability of getting such a sample mean under \\(\\mathcal{H_O}\\) is very small \\(\\rightarrow\\) my hypothesis is very unlikely to be valid \\(\\rightarrow\\) the true mean is probably not 10. 5.4.4.2 Student test A test is defined by its null hypothesis \\(\\mathcal{H_0}\\), the contrapositive being \\(\\mathcal{H_1}\\). The general procedure is to set \\(\\mathcal{H_0}\\) in a way that we can build a test statistic whose distribution we can derive. In general, statistician chose a null hypothesis such that they can build a statistic for which they know the distribution \\(\\rightarrow\\) they can compute probability for a specific value to occur. For the example, we’ll focus on the Student test. The test is meant to check whether the true value of the mean is equal to a specific value. Example, I know that the average age of the austrian population is 43.2 years. I want to test whether our customers have the same average age. Our test is the following : \\(\\mathcal{H_0}\\) : The average distance of activities is 20 \\(\\mathcal{H_1}\\) : The average distance of activities is not 20 The next important parameter of a test if \\(\\alpha\\), the Thanks to the CLT, we can build the following test statistic : \\[T = \\sqrt{n} \\dfrac{\\bar{A}-43.2}{\\sigma} \\hookrightarrow \\mathcal{N}(0,1)\\] Problem : we don’t know the value of the true standard deviation. The final test statistic is distributed with a Student distribution : \\[T = \\sqrt{n} \\dfrac{\\bar{A}-43.2}{\\hat{s}} \\hookrightarrow \\mathcal{St}_{n-1}\\] where \\(\\hat{s}^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (X-\\bar{X})^2\\) is the unbiased estimator of the variance. 5.4.4.2.1 Implementation in R : t.test(dat$distance/1E5,mu=20) ## ## One Sample t-test ## ## data: dat$distance/1e+05 ## t = -1.3698, df = 5506, p-value = 0.1708 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ## 18.58573 20.25079 ## sample estimates: ## mean of x ## 19.41826 5.4.4.2.2 Interpretation : If you have one thing to remember : the p-value is the probability to be wrong while rejecting \\(\\mathcal{H_0}\\). In our case, this probability is very small, meaning that rejecting the fact that our customers’ avergae age is equal to 43.2 is the good option \\(\\Rightarrow\\) our customers’ average age differs significantly from the austrian average. In short : all you have to know what the null hypothesis is and make your decision depending on the p-value. In general, we reject the null is p-value &lt; 5%, but you can decide to be more demanding and chose a lower threshold if you don’t want to make a mistake. Note : If you want to dig deeper, you can test whether the mean is strictly greater than a specific value using the parameter alternative of the t-test function. 5.4.4.3 Student test to compare group means You can also use the Student test to check whether the means of two sub-populations are equal or not. In this case, the tests checks whether the difference in means differs significantly from 0. We’ll check in this example if average age differs between churners and non churners. For that we have to extract the churners and non churners’ age in two separate vectors and run the test on those. bike &lt;- filter(dat,is_bike) %&gt;% pull(distance) non_bike &lt;- filter(dat,!is_bike) %&gt;% pull(distance) t.test(bike,non_bike) ## ## Welch Two Sample t-test ## ## data: bike and non_bike ## t = 23.825, df = 4586.7, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1809061 2133476 ## sample estimates: ## mean of x mean of y ## 2998157 1026888 In this case, the null is “the difference in means is 0” and the p-value is very very small (almost 0) \\(\\rightarrow\\) we can reject the null without second thoughts, which means there is a significant difference between the 2 sub-groups regarding average age. 5.4.4.4 Back to our \\(\\chi^2\\) Remember the \\(\\chi^2\\) test is used to assess if 2 categorical variables are independant or not. The null hypothesis in this test is “both variable are independant”. To check that, a test statistic, \\(D^2\\) (X-squared in R output) is built, and under \\(\\mathcal{H_0}\\), it is distributed with a \\(\\chi^2\\) probability distribution. We can can then test the validity of the null depending on the test value. Let’s check if there is a relationship between activity and distance in bins : tab &lt;- table(dat$qual_distance,dat$activity_recoded) tab ## ## Bike Other Run Swim ## Average 530 48 523 0 ## Long 297 39 764 1 ## Short 186 34 311 554 ## Very long 986 43 70 3 ## Very short 558 111 88 368 tab %&gt;% chisq.test() ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 3282.8, df = 12, p-value &lt; 2.2e-16 In this case, the p-value is much higher than 5% \\(\\Rightarrow\\) the probability to be wrong by rejecting the null is 31%… It is not reasonable to reject the null and we can consider that the two variables are independent, meaning churn and gender have no connection. 5.4.5 Other estimators : maximum of likelihood We saw how that the sample mean is a good estimator or the population mean. If you assume that the underlying probability distribution of the variable of interest is something else than a normal distribution, you can be interested in estimating another parameter than the mean. Let’s say we want to estimate the \\(s\\) parameter of the Zipf’s law. In this case, you can use the maximum likelihood estimator (MLE). The likelihood is the joint probability of observing the sample I got (which is the product of individual probabilities). Since we state that the underlying probability distribution is Zipf’s law, we can express the likelihood as a function of the \\(s\\) parameter. The idea of the MLE is since this sample occurred, it was the most likely to happen, hence the value of \\(s\\) is such that it maximizes the likelihood for my sample. Finding the maximum value is something for which we have a few algorithms ! For this estimator, we also know some asymptotic properties that allows to build tests, which you can interpret in the same manner. Namely, you have three tests : Wald test Likelihood ratio test LM test (Lagrange multiplier) All those test statistics are distributed according to a \\(\\chi^2\\) distribution MLE is also used for machine learning/econometrics, especially when relationships are non linear \\(\\Rightarrow\\) logistic regression. Notes : Sample mean is the MLE for the exponential family (eg gaussian distribution) MLE is a subset of M-estimators 5.4.6 Exercises : interprete a test you don’t know In time series analysis, it is crucial to check if a time serie is stationary or not. Stationarity means that both mean and variance are constant over time (no drift). If it’s the case, it’s easier to model it. To check that, there are several tests called “unit-root” tests (if there is a unit root, the serie is not stationary). In this example, I simulate a time series and run the Phillips-Perron test. The alternative hypothesis is specified in the output ; please interprete the result. library(tseries) # generation of a random walk Xt &lt;- cumsum(rnorm(100)) plot(Xt,type=&quot;l&quot;,main=&quot;Random time serie&quot;,col=&quot;blue&quot;) pp.test(Xt) ## ## Phillips-Perron Unit Root Test ## ## data: Xt ## Dickey-Fuller Z(alpha) = -9.3328, Truncation lag parameter = 3, p-value ## = 0.5745 ## alternative hypothesis: stationary "],
["multivariate-analysis-and-dimension-reduction.html", "Chapter 6 Multivariate analysis and dimension reduction 6.1 Multivariate analysis 6.2 Multivariate analysis and dimension reduction 6.3 Dimension reduction 6.4 Bonus : dashboards and reports", " Chapter 6 Multivariate analysis and dimension reduction 6.1 Multivariate analysis 6.1.1 Advanced visualization 6.1.1.1 The grammar of graphics The grammar of graphics was introduced in 2005 by Wilkinson and Leland as a general framework for graphical representation of data. It was adapted by Hadley Wickham in the R package ggplot2. It presents a unique foundation for producing almost every quantitative graphic found in scientific journals, newspapers, statistical packages, and data visualization systems. You can check the paper from Wickham When ploting data, one has to define : What are the aesthetics \\(\\rightarrow\\) the dimensions you want to represent What is the geometry you want to use \\(\\rightarrow\\) the kind of plot you want Ploting options (that come with default values) : scales of the axis fonts and colors for text labels (title, axis titles…) In short, what you have to find is the right combination of aesthetics and geometry that best represent the data. To find the recommended combination, don’t forget to use From data to viz 6.1.1.2 Playing with aesthetics You must define at least 1 dimension for the plot, either continuous or categorical for the x axis. Then you can increase the number of dimensions (ie columns of the data frame) you want to represent : x and y for the axis size : optional (integer) dimension that will represent an additional number color/fill : optional (categorical) dimension reprensented by a color. Color is for line/point geoms, fill for bars/heatmap geoms linetype : optional (categorical) : different type of lines (solid, dotted, dashed…). Only for geometries using lines shape : optional (categorical) : variable that will make the shape of the dot vary. Only for point geometries alpha : optional (continuous) : the transparency of the dots (the lower the value, the more transparent the dot) … More about the aesthetics here You can also use the facet_wrap() or facet_grid() functions to add up to 2 more dimensions with categorical variables (see demo) 6.1.1.3 Geometries Once you chose the variables (dimensions) you want to plot, you have to chose the geometry, that highly depends on the type of variable (numerical or character/factor). Don’t forget to reffer to the website from data to viz if you need some inspiration. A short list of most common geometries : geom_bar for barplots geom_histogram for histograms geom_jitter for scatter plots geom_boxplot &amp; geom_violin for compared density plots geom_tile for heatmaps geom_line for time series geom_text or geom_label for text (annotations) geom_hline and geom_vline for horizontal and vertiacal lines …… # From ggplot(dat_clean,aes(x=distance)) + theme_minimal() ggplot(dat_clean,aes(x=distance)) + geom_histogram()+ theme_minimal() ggplot(dat_clean,aes(distance,avgSpeed)) + geom_jitter()+ theme_minimal() ggplot(dat_clean,aes(distance,avgSpeed,color=activity_recoded)) + geom_jitter()+ theme_minimal() # To ggplot(dat_clean,aes(x=avgSpeed,y=calories,size=duration,color=qual_distance,shape=qual_avgHr)) + geom_jitter() + facet_wrap(.~ activity_recoded,scales = &quot;free&quot;)+ theme_minimal() # Or maybe ggplot(dat_clean,aes(x=avgSpeed,y=elevationGain,size=calories,color=duration)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; )+ theme_minimal() 6.1.1.4 Important options With ggplot, one can make publishable graphics that don’t need to be modified in another software. For that, the most useful functions are : scale_xx_yy : functions that allow you to tweak the axis’ scale, the colors used by either color or fill aesthetics (eg : scale_color_manual()) and other options. labs : allows you to proper label title, axis’ titles, legend titles… theme : allows you to tweak general parameters for the plot (font family, font size, margins, background colors…). You have several theme_xx() functions already defined with different default values for those parameters (eg theme_minimal() or theme_void()) guides : allows you to modify the legend entries Whats you can also do is recode the levels of the factor variables to make them more understanble or reorder them if you want them to be displayed in a specific order. See forcats::fct_recode() and forcats::fct_reorder() Hint : when working with strings, you can force a string to be split in 2 rows with \\n Here is an example : dat_clean %&gt;% ggplot(aes(x=avgSpeed,y=elevationGain,size=calories,color=duration,size=distance)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; ) + theme_minimal() + labs(title=&quot;Great insights&quot;,y=&quot;Total evelation&quot;,x=&quot;Average speed&quot;, color=&quot;Duration&quot;,size=&quot;Total distance&quot;) + # scale_color_manual(values = c(&quot;magenta&quot;,&quot;orange&quot;)) + scale_size_continuous(labels=scales::comma) + scale_y_continuous(labels=scales::comma) + scale_x_continuous(labels=scales::comma)+ theme_minimal() 6.1.1.5 Some tricks with ggplot This approach (grammar of graphics) is very coherent but makes it sometimes difficult. For example, how can I represent the distribution of several variables (and not the distribution of one variable according to different sub-groups -meaning that there is a second dimension -) ? 6.1.1.5.1 The cheater way You can brute-force the graph by superposing different geometries. But first, you’ll have to standardize the variables (they don’t have the same scale). Remember the usage to across to apply a function to a selection of variables. mutate(dat_clean,across(where(is.numeric), function(xx) (xx-mean(xx,na.rm=T))/sd(xx,na.rm=T))) %&gt;% ggplot() + geom_density(aes(distance),color=&quot;blue&quot;) + geom_density(aes(duration),color=&quot;red&quot;)+ theme_minimal() This solution can be also used if you want to superpose different geometries (bars and lines, bars and hlines,…), and is in this case legal :D 6.1.1.5.2 Do it with the tidy philosophy You can reformulate this task as : I want to represent the distribution of one unique variable for 3 subgroups : age, churn and usage. I have to reshape the data first to create such a variable. For that I will use tidyr::pivot_longer() which helps me to transform columns into rows reshaped &lt;- select(dat_clean,duration,distance) %&gt;% pivot_longer(cols = everything(),names_to=&quot;latent_variable&quot;,values_to=&quot;values&quot;) reshaped ## # A tibble: 10,250 x 2 ## latent_variable values ## &lt;chr&gt; &lt;dbl&gt; ## 1 duration 3640221. ## 2 distance 300000 ## 3 duration 15064570. ## 4 distance 12293220. ## 5 duration 10382433. ## 6 distance 8360927. ## 7 duration 5393830. ## 8 distance 1792890. ## 9 duration 3626825. ## 10 distance 3492347. ## # … with 10,240 more rows Now I can use the latent_variable variable as a dimension in a classic ggplot statement, with a prior standardization group_by(reshaped,latent_variable) %&gt;% mutate(values=(values-mean(values,na.rm = T))/sd(values,na.rm = T)) %&gt;% ggplot(aes(values,color=latent_variable)) + geom_histogram()+ theme_minimal() Or you can even skip the standardization step thanks to facets : ggplot(reshaped,aes(values)) + geom_histogram() + facet_wrap(.~latent_variable,scales = &quot;free&quot;)+ theme_minimal() 6.1.1.5.3 Combine different plots with ggpubr ggpubr makes your life much easier to make publication-ready graphics. It allows you for example to combine several ggplot graphics in a grid, regardless of any latent dimension that facet_grid would require. For that you have to store the graphics and “replay” them in a defined grid generated by ggarrange() # install.packages(&quot;ggpubr&quot;) require(ggpubr) gg1 &lt;- ggplot(dat_clean,aes(distance)) + geom_histogram() + labs(title = &quot;Age distribution&quot;)+ theme_minimal() gg2 &lt;- ggplot(dat_clean,aes(x=avgSpeed,y=elevationGain,size=calories,color=duration,size=distance)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; ) + theme_minimal() + labs(title = &quot;Beautiful but useless plot&quot;)+ theme_minimal() ggarrange(gg1,gg2,ncol = 2,widths = c(1,2)) There are a lot of options in this function (common legend, height and width of each plot,…). You can check the full documentation of this package. Note that you can mix tables and graphics with this function. Tables can be rendered as ggplot object with ggpubr::ggtexttable() 6.1.2 Easily explore an entire dataset Now, you know how to to one graph including several dimensions. To explore a new dataset and identify the correlations between them, you can visualize at a glance all variables in a datasets and their correlations with GGally 6.1.2.1 Scatter plot matrix The scatter plot matrix shows the correlations between all variables and helps you to spot dependencies between them. We can use either the basic plot() function on the dataframe or the GGally package which provides extensions to ggplot. Side note : the ggpairs function does a lot of computation and can take a lot of time ! \\(\\rightarrow\\) if your dataset is large, you should run it only on a sample of the observations with dplyr::sample_n() or dplyr::sample_frac(). select(dat_clean,distance,duration,avgHr,avgSpeed,avgPower, calories,elevationGain,avgBikeCadence,activity_recoded) %&gt;% plot() # install.packages(&quot;GGally&quot;) require(GGally) select(dat_clean,distance,duration,avgSpeed, calories,activity_recoded) %&gt;% GGally::ggpairs() 6.1.2.2 Correlation plots select(dat_clean,distance,duration,avgHr,avgSpeed,avgPower, calories,elevationGain,avgDoubleCadence,activity_recoded) %&gt;% GGally::ggcorr(geom = &quot;circle&quot;) 6.2 Multivariate analysis and dimension reduction In this section, we will focus on the bike activities, which have the highest number of metrics. However, before we can go further, we have to deal with missing data and scale them to avoid that the column with a large order of magnitude are over-weighted. 6.2.1 Imputation So far, we have ignored the missing values because we were computing summary statistics on one or 2 variables. The problem when taking into account more columns is that the probability of having one missing value on one of these features is higher, and therefore the risk that the whole observation is ignored increases. Every observation should still contain some original information that an analysis (or a model) should reflect. To avoid to ignore to many observations because of missing data we perform imputation, meaning that we replace the missing value with a true value. Many methods can be used to that end : For numeric variables Imputation with a random value from the sample Imputation with the mean/median Imputation with the nearest neighbours (k-nn) Hotdeck Imputation with a model (regression) For categorical variables : Imputation with random category selection Imputation with the most frequent category (total or of the neighbours) Hotdeck Model-based imputation The challenge here is to chose between “reflecting the instance’s originality” or “not creating noise” The following code counts the number of missing values per column and does a simple mean or median imputation. We will practice hte imputation via regression in the final chapter’s exercises. dat_bike &lt;- filter(dat_clean,is_bike) # Bike activities sapply(dat_bike,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 932 932 ## name activityType userProfileId ## 229 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 1262 0 ## startTimeLocal duration distance ## 0 0 1 ## elevationGain elevationLoss avgSpeed ## 175 176 2 ## maxSpeed avgHr maxHr ## 495 665 664 ## calories startLongitude startLatitude ## 2 583 583 ## aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 1376 0 0 ## elapsedDuration movingDuration anaerobicTrainingEffect ## 1189 1528 2021 ## deviceId minTemperature maxTemperature ## 852 1000 1000 ## minElevation maxElevation locationName ## 1018 1018 1781 ## maxVerticalSpeed lapCount endLongitude ## 1708 1642 2186 ## endLatitude activeSets totalSets ## 2186 1855 1855 ## totalReps purposeful autoCalcCalories ## 1855 0 1246 ## favorite pr elevationCorrected ## 0 0 871 ## atpActivity parent maxRunCadence ## 1828 474 2491 ## steps avgVerticalOscillation avgGroundContactTime ## 2491 2492 2492 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 2491 2289 2492 ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence ## 2492 2491 2491 ## avgPower avgBikeCadence maxBikeCadence ## 1832 1188 1188 ## strokes normPower avgLeftBalance ## 1197 1832 1896 ## avgRightBalance max20MinPower trainingStressScore ## 1896 1838 1881 ## intensityFactor lactateThresholdBpm lactateThresholdSpeed ## 1881 2492 2492 ## avgStrokes activeLengths avgSwolf ## 2492 2311 2492 ## poolLength avgStrokeDistance avgSwimCadence ## 2492 2492 2492 ## maxSwimCadence maxFtp workoutId ## 2492 2435 2381 ## decoDive parentId avgVerticalSpeed ## 2492 2479 2492 ## maxDepth avgDepth surfaceInterval ## 2492 2492 2492 ## floorsDescended bottomTime start_time ## 2492 2492 0 ## date is_bike is_run ## 0 0 0 ## activity_recoded qual_distance qual_avgHr ## 0 0 914 For this use case, we will use a median imputation, because for some of the variables, the mean would not make sense (eg longitude and latitude). dat_bike_imp &lt;- mutate(dat_bike,across(where(is.numeric), function(xx) ifelse(is.na(xx),mean(xx,na.rm=T),xx))) sapply(dat_bike_imp,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 932 932 ## name activityType userProfileId ## 229 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 1262 0 ## startTimeLocal duration distance ## 0 0 0 ## elevationGain elevationLoss avgSpeed ## 0 0 0 ## maxSpeed avgHr maxHr ## 0 0 0 ## calories startLongitude startLatitude ## 0 0 0 ## aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 0 0 0 ## elapsedDuration movingDuration anaerobicTrainingEffect ## 0 0 0 ## deviceId minTemperature maxTemperature ## 852 0 0 ## minElevation maxElevation locationName ## 0 0 1781 ## maxVerticalSpeed lapCount endLongitude ## 0 0 0 ## endLatitude activeSets totalSets ## 0 0 0 ## totalReps purposeful autoCalcCalories ## 0 0 1246 ## favorite pr elevationCorrected ## 0 0 871 ## atpActivity parent maxRunCadence ## 1828 474 0 ## steps avgVerticalOscillation avgGroundContactTime ## 0 2492 2492 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 2491 0 2492 ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence ## 2492 0 0 ## avgPower avgBikeCadence maxBikeCadence ## 0 0 0 ## strokes normPower avgLeftBalance ## 0 0 0 ## avgRightBalance max20MinPower trainingStressScore ## 0 0 0 ## intensityFactor lactateThresholdBpm lactateThresholdSpeed ## 0 2492 2492 ## avgStrokes activeLengths avgSwolf ## 2492 0 2492 ## poolLength avgStrokeDistance avgSwimCadence ## 2492 2492 2492 ## maxSwimCadence maxFtp workoutId ## 2492 0 2381 ## decoDive parentId avgVerticalSpeed ## 2492 2479 2492 ## maxDepth avgDepth surfaceInterval ## 2492 2492 2492 ## floorsDescended bottomTime start_time ## 2492 2492 0 ## date is_bike is_run ## 0 0 0 ## activity_recoded qual_distance qual_avgHr ## 0 0 914 Let’s do the same for categorical (maximum frequency), even though it is not mandatory for PCA. most_freq_cat &lt;- function(xx) { tab &lt;- table(xx) return(names(tab[which.max(tab)])) } dat_bike_imp &lt;- mutate(dat_bike_imp,across(where(is.character), function(xx) coalesce(xx,most_freq_cat(xx)))) sapply(dat_bike_imp,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 0 0 ## name activityType userProfileId ## 0 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 0 0 ## startTimeLocal duration distance ## 0 0 0 ## elevationGain elevationLoss avgSpeed ## 0 0 0 ## maxSpeed avgHr maxHr ## 0 0 0 ## calories startLongitude startLatitude ## 0 0 0 ## aerobicTrainingEffect avgFractionalCadence maxFractionalCadence ## 0 0 0 ## elapsedDuration movingDuration anaerobicTrainingEffect ## 0 0 0 ## deviceId minTemperature maxTemperature ## 0 0 0 ## minElevation maxElevation locationName ## 0 0 0 ## maxVerticalSpeed lapCount endLongitude ## 0 0 0 ## endLatitude activeSets totalSets ## 0 0 0 ## totalReps purposeful autoCalcCalories ## 0 0 1246 ## favorite pr elevationCorrected ## 0 0 871 ## atpActivity parent maxRunCadence ## 1828 474 0 ## steps avgVerticalOscillation avgGroundContactTime ## 0 2492 2492 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 0 0 2492 ## avgGroundContactBalance avgDoubleCadence maxDoubleCadence ## 2492 0 0 ## avgPower avgBikeCadence maxBikeCadence ## 0 0 0 ## strokes normPower avgLeftBalance ## 0 0 0 ## avgRightBalance max20MinPower trainingStressScore ## 0 0 0 ## intensityFactor lactateThresholdBpm lactateThresholdSpeed ## 0 2492 2492 ## avgStrokes activeLengths avgSwolf ## 2492 0 2492 ## poolLength avgStrokeDistance avgSwimCadence ## 2492 2492 2492 ## maxSwimCadence maxFtp workoutId ## 2492 0 0 ## decoDive parentId avgVerticalSpeed ## 2492 0 2492 ## maxDepth avgDepth surfaceInterval ## 2492 2492 2492 ## floorsDescended bottomTime start_time ## 2492 2492 0 ## date is_bike is_run ## 0 0 0 ## activity_recoded qual_distance qual_avgHr ## 0 0 0 # replacements &lt;- select(dat_bike_imp,activityId,where(is.character)) %&gt;% # pivot_longer(-activityId,names_to=&quot;name&quot;,values_to=&quot;val&quot;) %&gt;% # filter(!is.na(val)) %&gt;% # group_by(name,val) %&gt;% # summarise(cat_nb=n()) %&gt;% # arrange(name,-cat_nb) %&gt;% # group_by(name) %&gt;% # summarise(most_freq=first(val)) %&gt;% # pivot_wider(names_from = name,values_from=most_freq) %&gt;% # rename_with(function(xx) paste0(xx,&quot;_imp&quot;)) 6.2.2 Normalization Normalization is the operation consisting in scaling the columns so that their unit do not matter in the end. For example, the distance in meter is much larger than the cadence or the power, which have totally different units. To normalize the columns and make them unit-less, there are several methods among which the most common are the following : Standardization : \\(X_i^{std} = \\dfrac{X_i-\\bar{X}}{\\sigma_X} \\rightarrow\\) mean 0 and standard deviation 1 Min-Max scaling : \\(X_i^{std} = \\dfrac{X_i-min(X)}{max(X)-min(X)} \\rightarrow\\) between 0 and 1 Robust standardization \\(X_i^{std} = \\dfrac{X_i-Q2(X)}{Q3(X)-Q1(X)} \\rightarrow\\) similar to the first option but robust to outliers You can check scikit-learn documentation to see what other options you have (and then search for their R implementation). Normalization is a mandatory step before fitting a model, in order to avoid that only one feature bears the majority of the variance and makes the model biased. 6.2.3 PCA Principal Components Analysis (PCA) is often seen by machine learning engineers as a dimension reduction technique, but it is also a very powerful tool to explore your data (and it was designed for that purpose \\(\\rightarrow\\) IQ test). PCA applies on numerical variables only and aims to create new synthetic and uncorrelated variables, princpipal components (as a linear combination of the original variables) such that the inertia (ie variance) is highly concentrated on a small number of variables. The mathematical problem is to find eigenvalues and eigenvectors of the correlation matrix. The eigenvectors represent the linear combination of the original variables needed to design thos new variables, and the eigenvalues the variance that each of these new value bears. Once those new variable have been found, graphical representations with few dimensions are possible. To apply PCA, we will use the FactoMineR package, very easy to use for multivariate analysis. Notes : You can have supplementary continuous or categorical variables. They won’t be used in the construction of the eigenvectors, but you will be able to place them in the newly defined vector space. By default, almost all implementations of PCA standardizes the data, so you do not have to do it by yourself, but check in the documentation of the function you use # install.packages(&quot;FactoMineR&quot;) require(FactoMineR) acp_dat &lt;- select(dat_bike_imp,deviceId,duration,distance,elevationGain,elevationLoss,avgSpeed,avgHr,calories, minTemperature,maxTemperature,lapCount,avgBikeCadence,avgPower,vO2MaxValue, max20MinPower) acp &lt;- PCA(acp_dat,graph = F,quali.sup = c(1)) What’s in this new object ? str(acp) ## List of 6 ## $ eig : num [1:14, 1:3] 4.71 2.03 1.65 1.31 1.26 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:14] &quot;comp 1&quot; &quot;comp 2&quot; &quot;comp 3&quot; &quot;comp 4&quot; ... ## .. ..$ : chr [1:3] &quot;eigenvalue&quot; &quot;percentage of variance&quot; &quot;cumulative percentage of variance&quot; ## $ var :List of 4 ## ..$ coord : num [1:14, 1:5] 0.802 0.906 0.475 0.487 0.661 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cor : num [1:14, 1:5] 0.802 0.906 0.475 0.487 0.661 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:14, 1:5] 0.644 0.821 0.225 0.237 0.437 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ contrib: num [1:14, 1:5] 13.67 17.43 4.78 5.03 9.28 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## $ ind :List of 4 ## ..$ coord : num [1:2492, 1:5] 7.44 1.79 -1.3 3.28 3.48 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2492] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:2492, 1:5] 0.6847 0.0898 0.139 0.396 0.4002 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2492] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ contrib: num [1:2492, 1:5] 0.4721 0.0274 0.0143 0.0916 0.1032 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2492] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ dist : Named num [1:2492] 9 5.99 3.48 5.21 5.5 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:2492] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ svd :List of 3 ## ..$ vs: num [1:14] 2.17 1.43 1.29 1.15 1.12 ... ## ..$ U : num [1:2492, 1:5] 3.43 0.826 -0.598 1.511 1.604 ... ## ..$ V : num [1:14, 1:5] 0.37 0.417 0.219 0.224 0.305 ... ## $ quali.sup:List of 5 ## ..$ coord : num [1:9, 1:5] 0.797 -0.466 0.288 0.187 -0.311 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:9] &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; &quot;3858417560&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:9, 1:5] 0.2472 0.0613 0.0381 0.0196 0.1877 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:9] &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; &quot;3858417560&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ v.test: num [1:9, 1:5] 0.52 -1.517 0.265 0.15 -1.386 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:9] &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; &quot;3858417560&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ dist : Named num [1:9] 1.604 1.882 1.475 1.338 0.719 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; &quot;3858417560&quot; ... ## ..$ eta2 : num [1, 1:5] 0.148 0.0795 0.0733 0.1403 0.044 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr &quot;deviceId&quot; ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## $ call :List of 10 ## ..$ row.w : num [1:2492] 0.000401 0.000401 0.000401 0.000401 0.000401 ... ## ..$ col.w : num [1:14] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ scale.unit: logi TRUE ## ..$ ncp : num 5 ## ..$ centre : num [1:14] 4.67e+06 2.98e+06 3.87e+04 3.76e+04 5.58e-01 ... ## ..$ ecart.type: num [1:14] 3.54e+06 3.50e+06 1.14e+05 1.12e+05 3.30e-01 ... ## ..$ X :&#39;data.frame&#39;: 2492 obs. of 15 variables: ## .. ..$ deviceId : Factor w/ 9 levels &quot;3454054575&quot;,&quot;3825981698&quot;,..: 9 9 2 9 9 9 9 2 2 9 ... ## .. ..$ duration : num [1:2492] 15064570 10382433 3626825 8509404 8428058 ... ## .. ..$ distance : num [1:2492] 12293220 8360927 3492347 7991330 7420050 ... ## .. ..$ elevationGain : num [1:2492] 200100 74500 16000 61900 111700 ... ## .. ..$ elevationLoss : num [1:2492] 196800 74200 0 61100 111400 ... ## .. ..$ avgSpeed : num [1:2492] 0.816 0.805 0.963 0.939 0.88 ... ## .. ..$ avgHr : num [1:2492] 144 117 138 140 140 126 138 128 120 151 ... ## .. ..$ calories : num [1:2492] 16345 8745 3084 9327 9126 ... ## .. ..$ minTemperature: num [1:2492] 19 18 18.3 21 22 ... ## .. ..$ maxTemperature: num [1:2492] 29 29 26.2 30 32 ... ## .. ..$ lapCount : num [1:2492] 25 17 1 16 15 19 17 1 1 36 ... ## .. ..$ avgBikeCadence: num [1:2492] 83 82 91 88 87 82 86 87 86 88 ... ## .. ..$ avgPower : num [1:2492] 260 202 212 263 259 210 263 213 197 240 ... ## .. ..$ vO2MaxValue : num [1:2492] 72 71 64 71 71 ... ## .. ..$ max20MinPower : num [1:2492] 375 243 221 287 311 ... ## ..$ row.w.init: num [1:2492] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ call : language PCA(X = acp_dat, quali.sup = c(1), graph = F) ## ..$ quali.sup :List of 5 ## .. ..$ quali.sup :&#39;data.frame&#39;: 2492 obs. of 1 variable: ## .. .. ..$ deviceId: Factor w/ 9 levels &quot;3454054575&quot;,&quot;3825981698&quot;,..: 9 9 2 9 9 9 9 2 2 9 ... ## .. ..$ modalite : int 9 ## .. ..$ nombre : num [1:9] 2 49 4 3 90 ... ## .. ..$ barycentre:&#39;data.frame&#39;: 9 obs. of 14 variables: ## .. .. ..$ duration : num [1:9] 5815565 3745269 4374888 6604228 3170817 ... ## .. .. ..$ distance : num [1:9] 4241698 3402440 3889997 3052773 2049856 ... ## .. .. ..$ elevationGain : num [1:9] 41595 36724 28707 49032 41914 ... ## .. .. ..$ elevationLoss : num [1:9] 41333 0 30194 46806 41646 ... ## .. .. ..$ avgSpeed : num [1:9] 0.701 0.904 0.812 0.264 0.622 ... ## .. .. ..$ avgHr : num [1:9] 145 132 125 134 127 ... ## .. .. ..$ calories : num [1:9] 7924 3533 2740 4260 3524 ... ## .. .. ..$ minTemperature: num [1:9] 18.3 18.3 18.3 18.3 18.3 ... ## .. .. ..$ maxTemperature: num [1:9] 26.2 26.2 26.2 26.2 26.2 ... ## .. .. ..$ lapCount : num [1:9] 10.15 1.29 10.15 10.15 10.15 ... ## .. .. ..$ avgBikeCadence: num [1:9] 95.5 90.8 84.1 85.3 91.1 ... ## .. .. ..$ avgPower : num [1:9] 241 228 241 241 241 ... ## .. .. ..$ vO2MaxValue : num [1:9] 64 64 64 64 64 ... ## .. .. ..$ max20MinPower : num [1:9] 272 259 272 272 272 ... ## .. ..$ numero : num 1 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;PCA&quot; &quot;list &quot; This is a list with several elements : eig contains the eigenvalues, and their share in the total inertia (PCA is performed by default on scaled variables \\(\\rightarrow\\) the trace of the diagonal matrix equals to the number of columns) ind and var refer to rows and columns. They both have the same elements : “coord” for the coordinate on each principal component, “contr” the contribution to the inertia of this row (resp column) to the inertia of the component, “cos2” the squared cosine (ie quality of projection) for the row/column quali.sup have the same elements than the previous ones (coordinates are the barycenter of each category of the qualitative variable) The first thing is to have a look at the eignevalues to see how well the PCA could summarize the information barplot(acp$eig[,2]) barplot(acp$eig[,3]) In our case, the first component bears 35% of the total inertia and the second 15%. Hence, the first factorial plane (the first 2 components) : let’s look at how the variable correlate with them : plot.PCA(acp,choix=&quot;var&quot;,col.var = &quot;blue&quot;) What you can read at once on this plot is how important each variable is to construct the new variables. As a matter of fact, contribution, cos2 and coordinates are almost the same in PCA, meaning that the closer to the unit disc the coordinate is, the high are also contribution and quality of representation. What we deduct from this graph : Distance, calories and duration have the largest values on the first component \\(\\rightarrow\\) they contribute largely to the first component, which bears the 35% of total variance \\(\\rightarrow\\) those variables are the most discriminant Those three variables are very close to each other, which means they are highly correlated The average cadence is on the other side of the first component \\(\\righarrow\\) it is negatively correlated with those variables There is a right angle between elevationGain and avgPower \\(\\rightarrow\\) the correlation is very small Let’s check the correlation matrix to be sure about what we are reading here select(acp_dat,-deviceId) %&gt;% cor() %&gt;% corrplot::corrplot() How about the individuals ? plot.PCA(acp,choix=&quot;ind&quot;) This graph is harder to read, but we still can identify outliers and understand wht they are. Anyway in the top-right corner, those are long rides with a lot of ups and downs ! The most extreme is 380343030 which is another measurement error (indoor cycling with 34000 elevation meters !) 6.2.4 Exercises Remove the measurement errors and re-run the PCA ; what are the changes ? Let’s focus the running activities : select relevant features, impute the missing values using k-nn, run a PCA and analyze the results 6.3 Dimension reduction Dimension reduction aims to reduce the size of the data ; most of the time the goal is to decrease the number of columns but it can also apply to rows (depending on your use case). Why reduce the dimension : Avoid the curse of dimensionality Remove undesired noise Avoid multicollinearity in the features 6.3.1 Use the results of the PCA After performing a PCA, you can select a limited number of principal components that you will use further in the modeling task. You can chose the number of components to keep with several criteria : average inertia (eigenvalue &gt; 1) minimal total inertia (eg 80%) elbow criteria : keep components before there is a “drop” in the eigenvalues barplot 6.3.2 Other inertia-based methods PCA is designed only for continuous variables, but you can use other methods for other types of data : MCA (Multiple Correspondence Analysis ) for categorical variables (if you have a mixture of continuous and categorical, you can discretize numerical variables and perform MCA) FDA (Functional Data Analysis for functions/curves) 6.3.3 t-SNE t-SNE ahs a completely different approach to dimension reduction : it does not aim to preserve the total variance but rather the distances between the observations. 6.3.4 A word about clustering 6.4 Bonus : dashboards and reports 6.4.1 Shiny and rmarkdown R offers 2 main packages that allow you to create dashboards and reports in HTML format and the latest feature of web technologies without knowing anything about HTML, CSS or other web-specific languages : Rmarkdown is markdown adapted to R, allows you to create standalone documents in either PDF or HTML format, and mixes regular word-processor features and R code. The flexdashboard package is an extension to Rmarkdown that helps to generate dashboards in HTML format. Those tools are essentially used to design reproducible documents (scientific reports, regularly updated reports/dashboards…). You can also create presentation with it. Shiny is a package to develop interactive web-apps. A shiny app requires an R engine to be rendered on the client side. In the course material, you’ll find one example for each package. You’ll easily find online material to develop your skills further with these tools For rmarkdown For shiny 6.4.2 Web-based graphics with plotly plotly is a graphical library that generate “interactive” web-based graphics. It is available for R, but also python and other programming languages. You can learn the syntax of this package too, but there is a very useful function, plotly::ggplotly() that translates your ggplot object into a plotly graphic. It works for most of the cases. # install.packages(&quot;plotly&quot;) require(plotly) ggplotly(gg1) This is very useful to embed in a rmarkdown document or a shiny app ! 6.4.3 Other packages &amp; widgets You can use a lot of other html widgets to incorporate in your report/shiny app "],
["reg.html", "Chapter 7 Linear and logistic regression 7.1 Linear regression 7.2 Logitic regression", " Chapter 7 Linear and logistic regression Regression is the first machine learning algorithm. It allows you to model a target variable \\(y\\) depending on a set of explanatory variables or features \\(X\\) such that \\(y=f(X) + \\epsilon\\) where \\(f\\) is a linear function (for linear regression). 7.1 Linear regression 7.2 Logitic regression "]
]
